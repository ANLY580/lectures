{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*ANYL 580: NLP for Data Analytics*\n",
    "\n",
    "# **Information Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topics\n",
    "\n",
    "* Syntax\n",
    "* Part-of-Speech\n",
    "* Extraction Tasks\n",
    "  * Keywords\n",
    "  * Multi-word Expressions\n",
    "  * Named Entity Recognition\n",
    "  * Referring Expressions\n",
    "  * Co-reference\n",
    "  * Relation Extraction\n",
    "* Annotation\n",
    "* SpaCy - Customizing extraction\n",
    "* Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Where are we?\n",
    "![](../images/linguistic-abstractions.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Syntax\n",
    "\n",
    "- Structure\n",
    "\t- Word order\n",
    "\t- Relations between words\n",
    "- Compositionality \n",
    "- Properties of categories - part of a grammar or belonging to words themselves?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Syntax encodes relations between words. It also determines the arrangement of words, to include general patterns of movement. For example, \n",
    "\n",
    "- The time is 4:30\n",
    "- What time is it?\n",
    "\n",
    "Where the wh-word is \n",
    "\"fronted\"\n",
    "\n",
    "- Very generally, the meaning of an expression is derived by the meaning of its parts.\n",
    "- A corollary to this is that syntactic operations result in meaning change.\n",
    "\n",
    "The problem with compositionality has to do with knowledge that speaker's and hearer's have that contribute to meaning. We'll talk more about this next week when we talk about inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Order and Apparent Movement\n",
    "\n",
    "![](../images/WH-trace.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This representation of \"wh-movement\" does not express  reality, but is only one of many theories of syntax.\n",
    "\n",
    "Linguists study patterns in language and have constructed theories that account for regularities across the world's languages. This diagram depicts a theory of grammar where meaning is expressed at a deep level differently than at the surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](../images/svo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Image from Wikipedia: https://en.wikipedia.org/wiki/Subject%E2%80%93verb%E2%80%93object\n",
    "\n",
    "There are indeed repeated patterns across languages and for this reason, syntacticians working under Chomskian theories (e.g., generative linguistics) refer to **principles and parameters** of language. Principles are expressed as abstract rules while parameters are functions that are either present or absent.\n",
    "\n",
    "There are a lot of problems with Chomskyan generative linguistics around ideas of innateness, language modularity, poverty of the stimulus, etc. But some of the ideas around grammars are still useful, in that there is certainly interaction between words and word structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Syntactic Theories\n",
    "\n",
    "- **Generative grammar** (emphasis on a grammar-based model)\n",
    "- **Dependency grammar** (emphasis on relations between words)\n",
    "- **Categorial grammar** (emphasis on properties of syntactic categories)\n",
    "- **Functional / Cognitive** grammar (emphasis on lexicon and schemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Both functional grammar and cognitive grammar approach language from a lexical perspective. \n",
    "\n",
    "Why am I telling you all of this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NLP Tasks around Structure\n",
    "\n",
    "https://nlpprogress.com\n",
    "\n",
    "Consituency parsing\n",
    "- Dependency parsing\n",
    "- Semantic parsing\n",
    "- Semantic role labeling\n",
    "- Shallow syntax\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Next week we'll talk more about some of these tasks as they pertain to meaning and understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part of Speech: Word Categories\n",
    "\n",
    "Closed class - Rarely new words added\n",
    "\t- Determiners\n",
    "\t- Conjunctions\n",
    "\t- Pronouns\n",
    "\t- Cardinal numbers\n",
    "\t- Etc.\n",
    "- Open class  - Productive (new words often added)\n",
    "\t- Nouns\n",
    "\t- Adjectives\n",
    "\t- Verbs\n",
    "\t- ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Integral to a discussion of compositionality and meaning are categories referred to as \"part of speech\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Phrasal Categories\n",
    "- Determined by the headword (e.g., noun, verb, adj, ...)\n",
    "\n",
    "- too **slowly** (AdvP)\n",
    "- The **man** (NP)\n",
    "- At **lunch** (PP)\n",
    "- **Run** a mile (VP)\n",
    "- Very **happy** (AdjP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The word 'phrase' can mean different things to different folks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part of Speech Categories in NLP\n",
    "\n",
    "Common tag sets:\n",
    "- **Brown** - about 80\n",
    "- **Penn Treebank** (WSJ); 45 tags\n",
    "- **Universal Dependencies**; 6 open class 8 closed\n",
    "- **OntoNotes**; variant of Penn Treebank\n",
    "\n",
    "https://spacy.io/api/annotation#pos-tagging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## POS.... use in Data Science?\n",
    "\n",
    "- Pattern sequences (e.g., in Regex)\n",
    "- Filtering (e.g., IR)\n",
    "- Features (e.g., NER as we will talk about below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Extraction\n",
    "\n",
    "* Keywords\n",
    "* Multi-word Expressions\n",
    "* Information Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What are Keywords?\n",
    "- **Index Term (IR perspective)** - used for retrieval. Could be a controlled vocabulary,  topics,TF-IDF with weighted words such as in titles.\n",
    "- **User-generated (Content creator perspective)** - e.g., Hashtags, terminology\n",
    "- **Linguistic-textual** - words that occur in text more often than by chance alone (e.g., PMI)\n",
    "\n",
    "*Condensed representation of the essential meaning of a document*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Key Phrases\n",
    "\n",
    "![](../images/rake1.png)\n",
    "![](../images/rake2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "From: [Automatic keyword extraction from individual documents]( https://pdfs.semanticscholar.org/5a58/00deb6461b3d022c8465e5286908de9f8d4e.pdf)\n",
    "\n",
    "- The hyphen-delimited terms on the bottom were extracted from an algorithm called RAKE.\n",
    "\n",
    "- RAKE begins keyword extraction on a document by parsing its text into a set of **candidate keywords**.\n",
    "  - Tokenize\n",
    "  - Use stop words as delimeters\n",
    "- Score (uses frequency and length of phrase)\n",
    "- Rank (top n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TextRank\n",
    "\n",
    "![](../images/trank.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Builds off the PageRank algorithm - Textrank is a \"graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph,by taking into account global information recursively computed from the entire graph,rather than relying only on local vertex-specific information\"\n",
    "\n",
    "- Mihalcea and Tarau assigned nouns and adjectives as vertices in the graph. Intuitively, verbs, prepositions, and other parts of speech are not generally as important when considering keywords. Verticies are connected by **weighted edges based on co-occurrence, or closeness, scores between words**. If two words appear within a certain number of words from each other in the passage, they are connected in the graph with a higher edge weight the closer they are.\n",
    "\n",
    "- This algorithm is used for both keyword and sentence extraction\n",
    "\n",
    "- Tokenize\n",
    "- Add syntactic filters\n",
    "- Use only unigrams\n",
    "- Add to uni-directed, un-weighted graph - edge is added between words that co-occur between a window of n words\n",
    "- Each vertex initialized to 1\n",
    "- Run modified \"pagerank\" until convergence (typically 20-30 times; dampening at .85)\n",
    "\n",
    "Mihalcea & Tarau (2004) - [TextRank:BringingOrderintoTexts](https://www.aclweb.org/anthology/W04-3252.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ranking (based on PageRank)\n",
    "\n",
    "\n",
    "![](../images/page-rank-formula.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Great explanation of how to use TextRank for sentence extraction (summarization)\n",
    "https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\n",
    "\n",
    "![](../images/text-rank-summ.png)\n",
    "- Simple implementation description using TF-IDF and not dense vectors: https://www.slideshare.net/andrewkoo/textrank-algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1567     1  minimal generating sets\n",
      "[minimal generating sets]\n",
      "0.1371     4  systems\n",
      "[systems, systems, systems, a system]\n",
      "0.1178     3  solutions\n",
      "[solutions, solutions, solutions]\n",
      "0.1164     1  linear diophantine equations\n",
      "[linear Diophantine equations]\n",
      "0.1077     1  nonstrict inequations\n",
      "[nonstrict inequations]\n",
      "0.1050     1  mixed types\n",
      "[mixed types]\n",
      "0.1044     1  strict inequations\n",
      "[strict inequations]\n",
      "0.1000     1  a minimal supporting set\n",
      "[a minimal supporting set]\n",
      "0.0979     1  linear constraints\n",
      "[linear constraints]\n",
      "0.0919     1  upper bounds\n",
      "[Upper bounds]\n",
      "0.0913     1  a minimal set\n",
      "[a minimal set]\n",
      "0.0804     1  components\n",
      "[components]\n",
      "0.0797     1  natural numbers\n",
      "[natural numbers]\n",
      "0.0797     1  algorithms\n",
      "[algorithms]\n",
      "0.0782     1  all the considered types systems\n",
      "[all the considered types systems]\n",
      "0.0768     1  diophantine\n",
      "[Diophantine]\n",
      "0.0697     2  compatibility\n",
      "[Compatibility, compatibility]\n",
      "0.0693     1  construction\n",
      "[construction]\n",
      "0.0668     1  the set\n",
      "[the set]\n",
      "0.0629     2  criteria\n",
      "[Criteria, These criteria]\n",
      "0.0588     1  the corresponding algorithms\n",
      "[the corresponding algorithms]\n",
      "0.0528     1  all types\n",
      "[all types]\n"
     ]
    }
   ],
   "source": [
    "# TextRank with SpaCy\n",
    "\n",
    "import spacy\n",
    "import pytextrank\n",
    "\n",
    "# example text\n",
    "text = \"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types.\"\n",
    "\n",
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "tr = pytextrank.TextRank()\n",
    "nlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# examine the top-ranked phrases in the document\n",
    "for p in doc._.phrases:\n",
    "    print(\"{:.4f} {:5d}  {}\".format(p.rank, p.count, p.text))\n",
    "    print(p.chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "https://pypi.org/project/pytextrank/\n",
    "\n",
    "... And also a nice explanation of how to implement from scratch (essentially). https://towardsdatascience.com/textrank-for-keyword-extraction-by-python-c0bae21bcec0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MWE\n",
    "\n",
    "*Expressions which are made up of at least 2 words and which can be syntactically and/or semantically idiosyncratic in nature. Moreover, they act as a single unit at some level of linguistic analysis.*\n",
    "\n",
    "- Lexicalized (act as words)\n",
    "  - Fixed (can't be varied); \"in short\"\n",
    "  - Semi-Fixed\n",
    "    - non-decomposable (can be inflected) - \"kick the bucket\", \"kicked the bucket\"\n",
    "    - compound - \"peanut butter\"\n",
    "    - proper name - \"San Franciso 49ers\", \"49ers\"\n",
    "  - Syntactically-Flexible\n",
    "    - \"call up\", \"call kim up\"\n",
    "- Institutionalized (conventions); \"salt and pepper\" (hair color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Examples from: https://aclweb.org/aclwiki/Multiword_Expressions\n",
    "\n",
    "Multi-word Expressions can be challenging for a number of reason. Most obvious, they are often idiomatic and non-compositional (semantically). \n",
    "\n",
    "This is a problem for current neural models of attention. \n",
    "\n",
    "Here's a purely statistical approach from Wall and Gries (2018): \n",
    "http://www.stgries.info/research/2018_AW-STG_MWEs-MERGE&AFL.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Information Extraction Sub-Tasks\n",
    "\n",
    "- Extraction of semantic content from task. Sub-tasks include:\n",
    "  - Named Entity Recognition (NER)\n",
    "  - Co-reference Resolution (and entity linking)\n",
    "  - Relation Extraction\n",
    "  - Event Extraction (& co-reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "![](../images/entity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "NER is a sub-task of information extraction. The task is to recognize (find) and classify named entity **mentions** in unstructured text into pre-defined categories such as the person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why mentions?\n",
    "\n",
    "![](../images/recognizing-entities.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](../images/ner-tags.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another Sequence Problem? \n",
    "\n",
    "![](../images/lample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Image from: Lample et al. (2016) Neural Architectures for Named Entity Recognition. First approach without hand-crafted features.\n",
    "\n",
    "\n",
    "Nice synopsis of related work. http://www.davidsbatista.net/blog/2018/10/22/Neural-NER-Systems/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NER challenges\n",
    "\n",
    "![](../images/ner-challenges.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DARPA MUC Challenge\n",
    "\n",
    "![](../images/muc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ACE Overview\n",
    "\n",
    "![](../images/ace.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ACE Types\n",
    "![](../images/ace2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NER SOTA\n",
    "\n",
    "![](../images/ner-sota.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "http://nlpprogress.com/english/named_entity_recognition.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NER... Solved?\n",
    "\n",
    "![](../images/ner-unsolved.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Referring Expressions\n",
    "\n",
    "![](../images/referring-expressions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recall that we talk about references in text as mentions and that they refer to some abstract entity not in the text. When a reference refers to a previous mention - we call it an **anaphor**. And when two mentioned refer to the same entity, they are said to be related by **co-reference**.\n",
    "\n",
    "Co-reference resolution is the task of determining whether two mentions co-refer. Sets of co-referring mentions are often called a co-referene chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Information (Cognitive) Status\n",
    "\n",
    "![](../images/barking-dog.jpg)\n",
    "\n",
    "A. I couldn't sleep last night. **It** kept me awake.\n",
    "\n",
    "B. I couldn't sleep last night. **That dog next door** kept me awake.\n",
    "\n",
    "C. I couldn't sleep last night. **The dog next door** kept me awake.\n",
    "\n",
    "D. I couldn't sleep last night. **A dog kept me awake** kept me awake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "From Gundel: \n",
    "http://web.stanford.edu/group/cslipublications/cslipublications/HPSG/2003/gundel.pdf\n",
    "\n",
    "We'll talk more about pragmatics next week, but because we're talking about entities and reference -- we'll be touching upon concepts in this space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](../images/cognitive-status.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Note that the bottom three categories are representations in the current center of attention in both the speaker and hearer's mind.\n",
    "\n",
    "In fact, when we talk about reference we MUST talk about inferencing. What you hear is decoded in parallel with inferences - or aspects of intended meaning that are left under-specified by the speaker. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Co-reference\n",
    "https://huggingface.co/coref/\n",
    "\n",
    "![](../images/co-ref.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "HuggingFace medium post: https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30\n",
    "\n",
    "This doesn't work so well... why not?\n",
    "I have two dogs. Shelby is larger than Lily, but Lily runs faster. My big poodle likes to sit in my lap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entity Resolution (or Linking)\n",
    "\n",
    "- **Map mentions to entities** (a knowledge-base, for example)\n",
    "- When mixed with structured data, often this process looks like:\n",
    "  - De-duplication (in a single data set)\n",
    "  - Match records across data sets\n",
    "  - Link to an entity\n",
    "\n",
    "![](../images/network-resolution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entity resolution is a task that overlaps with NLP.\n",
    "\n",
    "Data Community DC has a nice tutorial around entity resolution - http://www.datacommunitydc.org/blog/2013/08/entity-resolution-for-big-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Relation Extraction\n",
    "\n",
    "1. Approach 1: Relations Mentions\n",
    "**Elevation Partners**, the $1.9 billion private equity group that was *founded* by **Roger McNamee**...\n",
    "\n",
    "- Is there a relation between entity mentions?\n",
    "- What is the relation? (founded)\n",
    "\n",
    "\n",
    "2. Approach 2: Relations\n",
    "**Roger McNamee**, a managing director at **Elevation Partners**,...\n",
    "![](../images/relation-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Image from: Riedel, Yao, and McCallum (2010), Modeling Relations and Their Mentions without Labeled Text.\n",
    "\n",
    "In approach 2, for each pair of entities mentioned together in at least one sentence, create one relation variable. For each pairs of entity mentions that appear in a sentence, create one relation mention variable and connect it to the relation variable.\n",
    "\n",
    "The standard corpus for distantly supervised relationship extraction is the New York Times (NYT) corpus, published in Riedel et al, 2010.\n",
    "\n",
    "This contains text from the New York Times Annotated Corpus with named entities extracted from the text using the Stanford NER system and automatically linked to entities in the Freebase knowledge base. Pairs of named entities are labelled with relationship types by aligning them against facts in the Freebase knowledge base. (The process of using a separate database to provide label is known as ‘distant supervision’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Customizing NER\n",
    "\n",
    "![](../images/ner-training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "https://spacy.io/usage/training\n",
    "\n",
    "To update an existing model, you will need some training and evaluation data. Your goal is to generalize -- not to fit to your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Annotation guidelines\n",
    "\n",
    "https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-entities-guidelines-v6.6.pdf\n",
    "\n",
    "Example:\n",
    "\n",
    "3.1.3 Fictional characters, names of animals, and names of fictional animals.\n",
    "\n",
    "Names of fictional characters are to be tagged; however, character names used as TV show titles will not be tagged when they refer to the show rather than the character name.\n",
    "\n",
    "- [Batman] has become a popular icon\n",
    "- [Adam West] s costume from Batman the TV series \n",
    "\n",
    "Names of animals are not to be tagged, as they do not refer to person entities.  The same is true for fictional animals and non-human characters.  These two examples do not yield mentions. \n",
    "- Morris the cat \n",
    "- Snuggle, the fabric softener bear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "To do this well, you need to specify an annotation specification. Here are the guidelines from ACE.\n",
    "https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-entities-guidelines-v6.6.pdf\n",
    "\n",
    "In it, are positive and negative examples of annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Annotations specs\n",
    "\n",
    "![](../images/ontonotes.png)\n",
    "\n",
    "https://spacy.io/api/annotation#named-entities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "SpaCy supports other models and annotation specs. \n",
    "\n",
    "This one represented in the slide is ontonotes.\n",
    "Weischedel et al. (2010) OntoNotes: A Large Training Corpus for Enhanced Processing, https://www.researchgate.net/publication/230876724_OntoNotes_A_Large_Training_Corpus_for_Enhanced_Processing\n",
    "\n",
    "This level of semantic representation goes far beyond the entity and relation types targeted in the ACE program, since every concept in the text is indexed, not just 100 pre-specified types. \n",
    "https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf\n",
    "\n",
    "The Ontonotes corpus v5 is a richly annotated corpus with several layers of annotation, including named entities, coreference, part of speech, word sense, propositions, and syntactic parse trees. These annotations are over a large number of tokens, a broad cross-section of domains, and 3 languages (English, Arabic, and Chinese). The NER dataset (of interest here) includes 18 tags, consisting of 11 types (PERSON, ORGANIZATION, etc) and 7 values (DATE, PERCENT, etc), and contains 2 million tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Annotation Tools\n",
    "\n",
    "- Prodigy - https://prodi.gy/\n",
    "- Doccano - https://doccano.herokuapp.com/\n",
    "- Brat - http://brat.nlplab.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SpaCy - Customizing Extraction\n",
    "\n",
    "![](../images/online-training-spacy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "You can tune your pipeline fairly easily. \n",
    "\n",
    "You’ll usually need to provide many examples to meaningfully improve the system — a few hundred is a good start, although more is better.\n",
    "\n",
    "Also, instead of sequences of Doc and GoldParse objects, you can use the “simple training style” and pass raw texts and dictionaries of annotations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Attention (again)\n",
    "\n",
    "![](../images/attention-example.png)\n",
    "\n",
    "https://distill.pub/2016/augmented-rnns/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "If you recall from last week, we talked a bit about the idea of a recurrent NN with attention. Recurrent NNs in-and-of themeselves. \n",
    "\n",
    "RNNs are not difficult to understand in the context of Machine Translation. The basic idea is that that the next item in a sequence gets as an input hidden states (context weights) from prior states.\n",
    "\n",
    "The RNN takes an input word vector and hidden state from the prior word and then the decoder essential unrolls this and does the same thing. Where attention comes into play is that the encoder passes ALL the hidden states to the decoder. Then the decoder considers all the hidden states and scores them.\n",
    "\n",
    "You read an article from Chris Olah, who also founded the site distill.pub. Let's take a look!\n",
    "\n",
    "One of the challenges with this approach are long-distance relations (like co-reference!). In fact, there are no great representations and architectures for this yet... but Transformers do improve performance for this sort of problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformers\n",
    "\n",
    "- Vaswani et al. (2017) Attention is all you need https://arxiv.org/abs/1706.03762\n",
    "- Overcome serial nature of RNNs by being more parallelizable\n",
    "- More accurate\n",
    "- Fast to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Tranformers let us eliminate RNNs by introducing a new architecture. Transformers have a mechanism for attention and also positional encoding within a fully end-to-end NN. Transformers are more parallelized, faster and more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Architecture\n",
    "![](../images/transformer-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This looks super complicated. But let's pull out the major concepts.\n",
    "\n",
    "- There is still an encoder and decoder, but with N identical layers. (This is 6 in the paper and basically means there are 6 iterations)\n",
    "- The three orange boxes are the attention parts. Without them, you have a standard feed-forward NN.\n",
    "- Without recurrent or convolutions, the model needs to know something about the position of a word. THis is the positional encoding.\n",
    "- Mult-head attention basically is a set of matrices. Let's look at them next.\n",
    "- The feedforward from the input to the attention module in the output is analogous to attention in the previous seq2seq model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](../images/attention-qkv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "For Jay Alammar's example of \"Thinking Machines\", we calculate matrices for each word.\n",
    "\n",
    "The Q, K, V values are all matrices that are learned. \n",
    "\n",
    "- Q is learned for the current word\n",
    "- K is learned for all the other words in the sentence\n",
    "- Q&K get combined to a representation of relevance\n",
    "\n",
    "Then V is returned as a vector for the whole sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](../images/attention-example2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The paper further refined the self-attention layer by adding a mechanism called “multi-headed” attention. This improves the performance of the attention layer in two ways:\n",
    "\n",
    "- It expands the model’s ability to focus on different positions. This useful if we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, we would want to know which word “it” refers to.\n",
    "- It gives the attention layer multiple “representation subspaces”. As we’ll see next, with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](../images/givenness-hierarchy.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This looks very analogous to the sort of neural model of linguistic saliency we talked about earlier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intro to SpCy\n",
    "\n",
    "- https://notebooks.azure.com/csbailey/projects/intro-nlp-spacy\n",
    "- [SpaCy 101](https://spacy.io/usage/spacy-101)\n",
    "- [Advanced SpaCy](https://course.spacy.io)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "file_extension": ".py",
  "jupytext": {
   "comment_magics": false,
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "271px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
