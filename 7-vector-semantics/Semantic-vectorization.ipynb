{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "ANYL 580: NLP for Data Analytics\n\n**Lecture 6: Semantic Vectorization**"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "TODO: Image of encoding documents as vectors"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The simplest encoding of semantic space is vocabulary. Thus, far we've used this insight in terms of the bag-of-words model. Documents that share many similar words are similar, in some sense."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "TODO: Image of toolkit logos",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Until now, we've touched on two different libraries for bag-of-words representation: NLTK and Scikit-Learn. Today we'll introduce a third and talk about some of the differences."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "TODO: screengrab semeval 2017"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We're also going to talk about your projects and concepts that J&M raised in Chapter 5. And we'll do this in the context "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Topic:\n\n- Semeval 2017\n- Features & Classifiers\n- Project #1\n- Tools\n- Vectorization\n - Frequency\n - One-hot encoding\n - TF-IDF\n - Log Odds\n - Distributed Representation\n- Maxent Classifiers\n- Learning and Optimization\n - Pipelines"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Semeval 2017"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "BOW: represent the document as a vector of words whose length is equal to the vocabulary of the corpus. [How does scikit learn do the mapping? Dictionary that maps tokens to vector positions?]"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Features and Classifiers"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "- Capitalization\n- Previous word\n- Unigrams, bigrams\n- POS\n- Features of head noun, preposition (parsing decisions)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Project #1"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Tools\nTODO: Table comparing NLTK, Scikit-Learn, Gensim\n- NTLK\n - big dependency\n - designed for teaching\n - \"word of mouth\" strong at tokenization (tested??)\n- Scikit-Learn\n - Not designed for text but with some NLP conveniences\n- Gensim\n - can serialize dictionaries and references in matrix market format (useful for multiple platforms)\n - no basic NLP conveniences"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "corpus = [\n    \"The elephant sneezed at the sight of potatoes.\",\n    \"Bats can see via echolocation. See the bat sneeze!\",\n    \"Wondering, she opened the door to the studio.\"\n]",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import nltk\nimport string\n\ndef tokenize(text):\n    stem = nltk.stem.SnowballStemmer('english')\n    text = text.lower()\n    \n    for token in nltk.word_tokenize(text):\n        if token in string.punctuation: continue\n        yield stem.stem(token)",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Vectorization"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Frequency\nTODO: image Frequency Vectors\n\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "- Each document is represented as a set of tokens and the value for each word position in the vector as its count.\n - straight count\n - normalized where each word is weighted by the total number of words in the document"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from collections import defaultdict\n\ndef vectorize(doc):\n    # set feature as 0\n    features = defaultdict(int)\n    for token in tokenize(doc):\n        features[token] +=1\n    return features\n\n# creates an iterable of vectorized documents\nvectors = map(vectorize, corpus)",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## One Hot Encoding"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## TF-IDF"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Log Odds"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Distributed"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Maxent Classifiers"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Learning and Optimization"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Pipelines"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Grid Search"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "base_numbering": 1,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}