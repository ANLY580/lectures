{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANLY 580 - NLP for Data Analytics\n",
    "Fall Semester 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modleing - Latent Dirichlet Allocation (LDA) and Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models import CoherenceModel, HdpModel\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify these parmeters for the local directory structre where the data os located. \n",
    "\n",
    "data_file_path = \"data/ANLY580/data\"\n",
    "temp_file_path = \"data/ANLY580/backup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the train and test data from Semval 2016 Task BD, which includes human annotated topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As shown below this example uses the Semval Gold training and test sets from subtask BD \n",
    "# which include human annotated tags for use in comparing topic modeling\n",
    "# interpreation of the content.\n",
    "\n",
    "# Load the data into a pandas dataframe. \n",
    "\n",
    "# Set to True to include additional data from the \"test\" set. \n",
    "INCLUDE_TRAIN = True  # Include the train set by default\n",
    "INCLUDE_TEST = False  # Set to True to also include the test set, results in longer run times \n",
    "\n",
    "# Read the train data into a pandas dataframe \n",
    "datafile = os.path.join(data_file_path, \"2017_English_final/GOLD/Subtasks_BD/twitter-2016train-BD.txt\")\n",
    "tweets1 = pd.read_csv(datafile, \n",
    "                     encoding = 'utf-8', \n",
    "                     sep = '\\t', \n",
    "                     header = None,\n",
    "                     index_col = False,\n",
    "                     names = ['msgid', 'topic', 'sentiment', 'Tweet'], \n",
    "                     dtype = {'msgid':str, 'topic':str, 'sentiment':str, 'Tweet':str})\n",
    "\n",
    "# Read the test data into a pandas dataframe\n",
    "datafile = os.path.join(data_file_path, \"2017_English_final/GOLD/Subtasks_BD/twitter-2016test-BD.txt\")\n",
    "tweets2 = pd.read_csv(datafile, \n",
    "                     encoding = 'utf-8', \n",
    "                     sep = '\\t', \n",
    "                     header = None,\n",
    "                     index_col = False,\n",
    "                     names = ['msgid', 'topic', 'sentiment', 'Tweet'], \n",
    "                     dtype = {'msgid':str, 'topic':str, 'sentiment':str, 'Tweet':str})\n",
    "\n",
    "\n",
    "if INCLUDE_TRAIN and INCLUDE_TEST:\n",
    "    tweets = pd.concat([tweets1, tweets2], ignore_index=True)\n",
    "elif INCLUDE_TRAIN:\n",
    "    tweets = tweets1\n",
    "else:\n",
    "    tweets = tweets2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many human annotated topics are in the data?\n",
    "\n",
    "human_topics = list(set(tweets['topic'].tolist()))\n",
    "print(\"Number of human topics in the data: {}\".format(len(human_topics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's look at the shape of the data, how many tweets are in the data set?\n",
    "print(\"Shape of the data: {}\".format(tweets.shape))\n",
    "\n",
    "# And take a look at the first 10 rows\n",
    "tweets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And take a look at the last 10 rows\n",
    "tweets.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the distributions of human annotated/tagged topics in the data via a barplot\n",
    "\n",
    "sns.set(rc={'figure.figsize':(12.7,9.27)})\n",
    "by_topic = sns.countplot(x='topic', data=tweets)\n",
    "\n",
    "for item in by_topic.get_xticklabels():\n",
    "    item.set_rotation(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the distributions of human annotated/tagged sentiment by topic via a barplot\n",
    "\n",
    "human_sentiment = list(set(tweets['sentiment'].tolist()))\n",
    "df_sentiment = tweets.groupby(['topic', 'sentiment'])['topic'].count().unstack('sentiment')\n",
    "topic_mixture = df_sentiment[human_sentiment].plot(kind='bar', stacked=True, legend = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the human anotated set of topics look like\n",
    "human_topics = list(set(tweets['topic'].tolist()))\n",
    "print(human_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the corpus for analysis and checking first 10 entries\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for i in range(len(tweets['Tweet'])):\n",
    "    tweet = tweets['Tweet'][i]\n",
    "    \n",
    "    # For topic modeling we will remove the url's for consdieration as terms in our topics\n",
    "    #tweet = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \"\", tweet)\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)\n",
    "    \n",
    "    # Uncomment the following to remove hashtags and mentions\n",
    "    #tweet = re.sub(r'#\\w+ ?', '', tweet)\n",
    "    #tweet = re.sub(r'@\\w+ ?', '', tweet)\n",
    "    \n",
    "    # OR, uncomment the following to remove the character tags for mentions and hashtags\n",
    "    #tweet = tweet.replace(\"@\", \"\").replace(\"#\", \"\")\n",
    "    \n",
    "    tweet = tweet.replace(\"&amp;\", \" \").replace(\"&gt;\", \"\").replace(\"&lt;\", \"\")\n",
    "    tweet = tweet.replace(\"(\", \"\").replace(\")\", \"\").replace(\".\", \"\").replace(\"?\", \"\").replace(\"!\", \"\").replace(\",\", \"\")\n",
    "    tweet = tweet.replace(\"/\", \" \").replace(\"=\", \"\").replace('\\\"', \"\").replace('*', '').replace(';', \"\")\n",
    "    tweet = tweet.replace(':', '').replace('\"', '')\n",
    "    tweet = re.sub(r'\\$[0-9]+', '', tweet)\n",
    "    tweet = re.sub(r'[0-9]+GB', '', tweet)\n",
    "    tweet = re.sub(r'[0-9]+', '', tweet)\n",
    "    tweet = re.sub(r'--+', ' ', tweet)\n",
    "    \n",
    "    corpus.append(tweet)\n",
    "\n",
    "# Dump out the first 10 tweets to see what the parsing has done\n",
    "corpus[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If our temp folder is not present, create it. \n",
    "\n",
    "TEMP_FOLDER = temp_file_path\n",
    "if os.path.exists(TEMP_FOLDER) == False:\n",
    "    os.mkdir(TEMP_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gensim LDA does not use the words directly when determining topics but instead uses ids as representations for the words. The mapping between ids and words is stored in a python dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have done some prep of the corpus we can perform some additonal word level processing to remove \n",
    "# extraneous tokens and common stopwords that do not contribute our discovery of topics in the corpus.\n",
    "\n",
    "# Define our stoplist for removing common words and tokenizing\n",
    "list1 = ['RT','rt', '&amp;', 'im', 'b4', 'yr', 'nd', 'rd', 'oh', \"can't\", \"he's\", \"i'll\",\n",
    "         \"i'm\", 'ta', \"'s\", \"c'mon\", 'th', 'st', \"that's\", \"they're\", \"i've\", 'am', 'pm']\n",
    "stoplist = stopwords.words('english') + list(string.punctuation) + list1\n",
    "#print(stoplist)\n",
    "\n",
    "# Remove tokens in the text that match our stoplist tokens and lower case all tokens\n",
    "texts = [[word for word in str(document).lower().split() if word not in stoplist] for document in corpus]\n",
    "\n",
    "# Create and save the dictionary, in case we want to reload it into another notebook\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save(os.path.join(TEMP_FOLDER, 'semval.dict'))  # store the dictionary, for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the averge length of the documents in the corpus?\n",
    "text_length = []\n",
    "for t in texts:\n",
    "    text_length.append(len(t))\n",
    "    \n",
    "tweets['doc_length'] = pd.Series(text_length)\n",
    "\n",
    "avg_doc_length = tweets['doc_length'].mean() \n",
    "median_doc_length = tweets['doc_length'].median()\n",
    "min_doc_length = tweets['doc_length'].min()\n",
    "max_doc_length = tweets['doc_length'].max()\n",
    "\n",
    "print(\"Average tweet length: {}\".format(avg_doc_length))\n",
    "print(\"Median tweet length: {}\".format(median_doc_length))\n",
    "print(\"Minimum tweet length: {}\".format(min_doc_length))\n",
    "print(\"Maximum tweet length: {}\".format(max_doc_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Found some anomalies in the input training data set that resulted in very long tweets.\n",
    "# Code below is a chedk that I corrected all the anomalies\n",
    "\n",
    "tweets_filtered = tweets[tweets['doc_length'] > 40]\n",
    "tweets_filtered.head()\n",
    "\n",
    "outliers = tweets_filtered['Tweet'].tolist()\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump out the dictionay to examine list of resulting tokens\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gensim LDA stores all the text for processing into a corpus object. All text is filtered through the previously constructed dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'semval.mm'), corpus)  # store to disk, for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model\n",
    "corpus_tfidf = tfidf[corpus]      # step 2 -- use the model to transform vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reference for lda parameters: https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the number of human annotated topics by commenting out the second total_topics assignment \n",
    "total_topics = len(human_topics)\n",
    "total_topics = 20\n",
    "\n",
    "# Experiment with the alpha assingment to investigate alpha parameter settings on topic assignments\n",
    "lda_alpha = 'auto' #learns asymmetic prior from the corpus\n",
    "lda_alpha = 'symmetric'\n",
    "#lda_alpha = 'asymmetric'  # sets alpha = 1 / number_of_topics\n",
    "#lda_alpha = np.full((total_topics), (0.05 * avg_doc_length) / total_topics)  # from NIH paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "lda = models.LdaModel(corpus, id2word = dictionary, num_topics = total_topics, iterations = 1000, alpha=lda_alpha)\n",
    "corpus_lda = lda[corpus] # Use the bow corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show first n=80 important words in the topics:\n",
    "lda.show_topics(total_topics, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the topic - term data into an python dictionary\n",
    "data_lda = {i: OrderedDict(lda.show_topic(i,20)) for i in range(total_topics)}\n",
    "data_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the ordered dictionary to load the data into a dataframe\n",
    "df_lda = pd.DataFrame(data_lda)\n",
    "df_lda = df_lda.fillna(0).T\n",
    "print(df_lda.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dataframe view of some of terms across topics\n",
    "\n",
    "df_lda.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyLDAvis.enable_notebook()\n",
    "#panel = pyLDAvis.gensim.prepare(lda, corpus, dictionary, mds='tsne')\n",
    "#panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have some idea of the topic distribution from the preceding cell but no real idea how lda matched the human annotated topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_tweets = tweets.filter(['msgid', 'topic', 'sentiment'], axis =1)\n",
    "se = pd.Series(texts)\n",
    "parsed_tweets['Tweet'] = se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_tweets.to_csv(os.path.join(data_file_path, \"parsed_tweets.csv\"), sep=\"\\t\")\n",
    "parsed_tweets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A check of the data types included in parsed_tweets\n",
    "\n",
    "#parsed_tweets.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the original documents back thru the model to infer the distribution of topics \n",
    "# according to the lda model\n",
    "\n",
    "topics = []\n",
    "probs = []\n",
    "max_to_show = 20\n",
    "\n",
    "for k, i in enumerate(range(len(parsed_tweets['Tweet']))):\n",
    "    bow = dictionary.doc2bow(parsed_tweets['Tweet'][i])\n",
    "    doc_topics = lda.get_document_topics(bow, minimum_probability = 0.01)\n",
    "    topics_sorted = sorted(doc_topics, key = lambda x: x[1], reverse = True)\n",
    "    topics.append(topics_sorted[0][0])\n",
    "    probs.append(\"{}\".format(topics_sorted[0][1]))\n",
    "    \n",
    "    # Dump out the topic and probability assignments for the first 20 documents\n",
    "    if k < max_to_show:\n",
    "        print(\"Document {}: {}\".format(k, topics_sorted))\n",
    "\n",
    "parsed_tweets['LDAtopic'] = pd.Series(topics)\n",
    "parsed_tweets['LDAprob'] = pd.Series(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resort the dataframe according to the human annotated topic and lda topic\n",
    "parsed_tweets.sort_values(['topic', 'LDAtopic'], ascending=[True, True], inplace=True)\n",
    "parsed_tweets.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the distributions of human annotated topics in the data via a barplot\n",
    "\n",
    "sns.set(rc={'figure.figsize':(12.7,9.27)})\n",
    "by_topic = sns.countplot(x='LDAtopic', data=parsed_tweets)\n",
    "\n",
    "for item in by_topic.get_xticklabels():\n",
    "    item.set_rotation(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resort the dataframe according to the the lda assigned topic and the human annotated topic\n",
    "\n",
    "parsed_tweets.sort_values(['LDAtopic', 'topic'], ascending=[True, True], inplace=True)\n",
    "parsed_tweets.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resort the dataframe according to the the lda assigned topic and the assocoiated probability\n",
    "parsed_tweets.sort_values(['LDAtopic', 'LDAprob'], ascending=[True, False], inplace=True)\n",
    "parsed_tweets.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do the topic distrubtions look like relative to the original human annotated/tagged topics\n",
    "\n",
    "df2 = parsed_tweets.groupby(['LDAtopic', 'topic'])['LDAtopic'].count().unstack('topic')\n",
    "topic_mixture = df2[human_topics].plot(kind='bar', stacked=True, legend = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do the topic distrubtions look like relative to the original human annotated/tagged sentiment\n",
    "\n",
    "human_sentiment = list(set(parsed_tweets['sentiment'].tolist()))\n",
    "df2 = parsed_tweets.groupby(['LDAtopic', 'sentiment'])['LDAtopic'].count().unstack('sentiment')\n",
    "topic_mixture = df2[human_sentiment].plot(kind='bar', stacked=True, legend = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A major question in using LDA for topic modeling is what is is the proper set of\n",
    "# hyperparmeters to generate the optimal set of topics for the coprus of documents\n",
    "# under examination. Gensim includes methods for computing the Perplexity and Topic \n",
    "# Coherence of a corpus. One appraoch to is to sample an LDA model for a range of \n",
    "# for perplexity and topic coherence and select the appropriate number of topics\n",
    "# from a point of minimum perplexity and maximium topic coherence.\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "perplexity_lda = []\n",
    "coherence_lda = []\n",
    "topic_count_lda = []\n",
    "\n",
    "for num_topics in range(15, 70, 5):\n",
    "    \n",
    "    print(\"Computing the lda model using {} topics\".format(num_topics))\n",
    "    \n",
    "    topic_lda = models.LdaModel(corpus,\n",
    "                                id2word = dictionary,\n",
    "                                num_topics = total_topics,\n",
    "                                iterations = 1000,\n",
    "                                alpha = lda_alpha)\n",
    "    corpus_lda = topic_lda[corpus] # Use the bow corpus\n",
    "    \n",
    "    topic_count_lda.append(num_topics)\n",
    "    \n",
    "    perplexity_lda.append(topic_lda.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "    # Compute Coherence Score\n",
    "    cm = CoherenceModel(model=topic_lda, corpus=corpus, dictionary=dictionary, coherence='u_mass')\n",
    "    coherence_lda.append(cm.get_coherence())\n",
    "    \n",
    "\n",
    "#cm = CoherenceModel(model=lda, corpus=corpus, dictionay=dictionary, coherence='c_v')\n",
    "#coherence_lda = cm.get_coherence()\n",
    "#print('\\nCoherence Score (c_v): ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the resulting data into a pandas dataframe\n",
    "topics_lda = pd.DataFrame({'perplexity': perplexity_lda,\n",
    "                           'coherence': coherence_lda},\n",
    "                         index = topic_count_lda)\n",
    "\n",
    "topics_lda.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = topics_lda.plot.line(subplots = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gensim also includes Hierarchical Dirichlet process (HDP). HDP is a powerful mixed-membership model for \n",
    "the unsupervised analysis of grouped data. Unlike its finite counterpart, latent Dirichlet allocation, \n",
    "the HDP topic model infers the number of topics from the data. Here we have used Online HDP, \n",
    "which provides the speed of online variational Bayes with the modeling flexibility of the HDP.\n",
    "\n",
    "See https://radimrehurek.com/gensim/models/hdpmodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a HDP model - default for hdp is 150\n",
    "hdpmodel = HdpModel(corpus=corpus, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdptopics = hdpmodel.show_topics(num_topics = 20, formatted=True)\n",
    "hdptopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdp_topics = hdpmodel.get_topics()\n",
    "hdp_topics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdpmodel.hdp_to_lda()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
