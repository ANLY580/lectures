{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "---\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    cell_markers: region,endregion\n",
    "    comment_magics: false\n",
    "    formats: ipynb,.pct.py:hydrogen,Rmd,md\n",
    "    text_representation:\n",
    "      extension: .py\n",
    "      format_name: hydrogen\n",
    "      format_version: '1.1'\n",
    "      jupytext_version: 1.1.5\n",
    "  kernelspec:\n",
    "    display_name: Python 3\n",
    "    language: python\n",
    "    name: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# If you are working in binder, you can comment any import statements in the blocks below.\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import regex\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week we saw Unicode challenges with the python **re** library and moved to the **regex** library. In fact, we lost tokens that we hadn't intended.\n",
    "\n",
    "We also learned a slightly more efficient way to manage a large regular expression (though the one below could still benefit from documentation). This pattern was created to handle social text like that from Twitter before the availabilty of fairly good \"Twitter-aware\" tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "\"(?:[¬∞\\p{Punctuation}\\p{Modifier_Symbol}\\p{Math_Symbol}}]+(?:[\\p{Letter}\\p{Number}][¬∞\\p{Punctuation}\\p{Modifier_Symbol}\\p{Math_Symbol}]+))\",\n",
    "\"(?:\\@+\\p{Letter}+)\",\n",
    "\"(?:\\:\\-\\p{Letter}+)\",\n",
    "\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",\n",
    "\"(?:norm__?[_A-Z]+)\",\n",
    "\"(?:\\p{Letter}\\.(?:\\p{Letter}\\.)+)\",\n",
    "\"(?:\\p{Letter}\\/\\p{Letter}(?:\\/\\p{Letter})*)\",\n",
    "\"(?:\\b(?:(?:http|ftp|file)s?:\\/\\/\\S+\\w+\\.\\w+\\.(?:\\w\\.)?(?:com|edu|gov|org|info|biz|mil|net)?(?:[a-z]{2})?\\S+)\\b)|(?:&(?:#?[0-9a-f]+|[a-z]+);)\",\n",
    "\"(?:\\b[0-9]*(?:1st|2nd|3rd|11th|12th|13th|[4-9]th)\\b)\",\n",
    "\"(?:[.+\\-]?\\p{Number}+(?:[.,\\-:\\/]*\\p{Number}+)*)\",\n",
    "\"(?:(?:n[\\'‚Äô]t\\b)|(?:[\\'‚Äô](?:[sdm]|(?:ld)|(?:ll)|(?:re)|(?:ve)|(?:nt))\\b))\",\n",
    "\"(?:[\\p{Letter}\\p{Mark}]+(?:[\\-\\'‚Äô][\\p{Letter}\\p{Mark}]+)*)\",\n",
    "\"(?:\\.\\.+|--+|__+|~~+|!!+|\\*\\*+|\\?\\?+|//+)\",\n",
    "\"(?:\\.\\.\\.+|---+|___+|~~~+|!!!+|\\*\\*\\*+|\\?\\?\\?+|///+)\",#\"(?:<\\w+\\/>)\",\n",
    "\"(?:[\\@]?\\p{Letter}+)\",\n",
    "#\"(?:\\p{Space}+)\",\n",
    "#\"(?:\\p{Separator}+)\",\n",
    "\"(?:\\p{Punctuation}+)\"\n",
    "]\n",
    "big_regex = ('|').join(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"ü•∞ Hey!  @sima #roadtrip from 09/09-9/27, aren't you in dude :-D  ?!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey', '!', '@sima', '#roadtrip', 'from', '09/09-9/27', ',', \"aren't\", 'you', 'in', 'dude', ':-D', '?!']\n"
     ]
    }
   ],
   "source": [
    "tokens = regex.findall(big_regex,input)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of advantages and disadvantages to tokenizers in NLTK. For example, \n",
    "- ToktokTokenizer() is very fast\n",
    "- MosesTokenizer() is backwards capable and can detokenize text\n",
    "- ReppTokenizer() is able to provide token offsets\n",
    "\n",
    "Actually - the MosesTokenizer seems to have been moved to: https://github.com/alvations/sacremoses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, for short texts (like those in twitter or other social media), there are some patterns that require specialized tokenizers and pre-processing steps.\n",
    "\n",
    "Below are the first few tweets from assignment 1 - Twitter English data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#ArianaGrande Ari By Ariana Grande 80% Full https://t.co/ylhCMETHHW #Singer #Actress https://t.co/lTrb1JQiEA\\n', 'Ariana Grande KIIS FM Yours Truly CD listening party in Burbank https://t.co/ClQIcx8Z6V #ArianaGrande\\n', 'Ariana Grande White House Easter Egg Roll in Washington https://t.co/jdjL9swWM8 #ArianaGrande\\n', '#CD #Musics Ariana Grande Sweet Like Candy 3.4 oz 100 ML Sealed In Box 100% Authenic New https://t.co/oFmp0bOvZy‚Ä¶ https://t.co/WIHLch9KtK\\n', 'SIDE TO SIDE üòò @arianagrande #sidetoside #arianagrande #musically #comunidadgay #lgbtüåà  #LOTB‚Ä¶ https://t.co/tEd8rftAxV\\n', \"Hairspray Live! Previews at the Macy's Thanksgiving Day Parade! https://t.co/GaFTqInolL #arianagrande #televisionnbc\\n\", '#LindsayLohan Is ‚ÄòFeeling Thankful‚Äô After Blasting #ArianaGrande For Wearing ‚Äò#TooMuch‚Ä¶ https://t.co/Acf8ogvPxd https://t.co/EOSHtG9ay4\\n', 'I hate her but... I love her songs Dammit ._.#ArianaGrande\\n', 'Ariana Grande „ÄêRight There ft. Big Sean„Äë#„Ç¢„É™„Ç¢„Éä #arianagrande https://t.co/4rZOKYDnut\\n', 'which one would you prefer to listen to for a whole day? üòçü§òüèº; i could never choose #arianagrande #IntoYou #SideToSide #songs #Poll\\n', 'Booty Baby Ari#ArianaGrande #PrincessAri #bootybaby #DangerousWomanTour #DangerousWoman‚Ä¶ https://t.co/YBspYl06Xn\\n', '#LindsayLohan backs out of a #Kettering holiday appearance, just after throwing shade at #ArianaGrande. #LiLohttps://t.co/G2sNgjVCHd\\n', 'My idols are #littlemix #justinbieber #arianagrande\\n', 'Ariana Grande - The Sims 3 - SimsDomination #ArianaGrande https://t.co/l2K2ihUpCg https://t.co/W7sIK84nOT\\n', '#Music #ArianaGrande-THE REMIX-JAPAN ONLY #CD E78 https://t.co/zOxb3hwBpg #Bestseller #Hit https://t.co/ibZScGfvl3\\n', '#Beauty #ArianaGrande-CHRISTMAS & CHILL-JAPAN ONLY #CD BONUS TRACK C94 https://t.co/pDp9gEqAW6 #Deals #TopSeller https://t.co/1cEhc7IqDj']\n"
     ]
    }
   ],
   "source": [
    "with open('data/social.txt', encoding=\"utf-8\") as file:\n",
    "    tweets=[]\n",
    "    data = file.readlines()\n",
    "    for tweet in data:\n",
    "        tweets.append(tweet)\n",
    "    print(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even for English short texts, there are potentially number of challenges for tokenization:\n",
    "- mentions/usernames\n",
    "- URLs, numbers\n",
    "- textual [emoticons](https://en.wikipedia.org/wiki/List_of_emoticons)\n",
    "- [emoji](https://en.wikipedia.org/wiki/Emoji)\n",
    "- words (including hyphenated words)\n",
    "- case-folding \n",
    "- punctuation \n",
    "- hashtags \n",
    "- non-English words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note for performance reasons, many tokenizers use **compiled regular expressions** which are cached and may result in substantial performance gain, depending on how often you use them and possibly how many you have.\n",
    "\n",
    "Check the source code here to examine:\n",
    "https://www.nltk.org/_modules/nltk/tokenize/casual.html#TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['#ArianaGrande', 'Ari', 'By', 'Ariana', 'Grande', '80', '%', 'Full', 'https://t.co/ylhCMETHHW', '#Singer', '#Actress', 'https://t.co/lTrb1JQiEA'], ['Ariana', 'Grande', 'KIIS', 'FM', 'Yours', 'Truly', 'CD', 'listening', 'party', 'in', 'Burbank', 'https://t.co/ClQIcx8Z6V', '#ArianaGrande'], ['Ariana', 'Grande', 'White', 'House', 'Easter', 'Egg', 'Roll', 'in', 'Washington', 'https://t.co/jdjL9swWM8', '#ArianaGrande'], ['#CD', '#Musics', 'Ariana', 'Grande', 'Sweet', 'Like', 'Candy', '3.4', 'oz', '100', 'ML', 'Sealed', 'In', 'Box', '100', '%', 'Authenic', 'New', 'https://t.co/oFmp0bOvZy‚Ä¶', 'https://t.co/WIHLch9KtK'], ['SIDE', 'TO', 'SIDE', 'üòò', '@arianagrande', '#sidetoside', '#arianagrande', '#musically', '#comunidadgay', '#lgbt', 'üåà', '#LOTB', '‚Ä¶', 'https://t.co/tEd8rftAxV'], ['Hairspray', 'Live', '!', 'Previews', 'at', 'the', \"Macy's\", 'Thanksgiving', 'Day', 'Parade', '!', 'https://t.co/GaFTqInolL', '#arianagrande', '#televisionnbc'], ['#LindsayLohan', 'Is', '‚Äò', 'Feeling', 'Thankful', '‚Äô', 'After', 'Blasting', '#ArianaGrande', 'For', 'Wearing', '‚Äò', '#TooMuch', '‚Ä¶', 'https://t.co/Acf8ogvPxd', 'https://t.co/EOSHtG9ay4'], ['I', 'hate', 'her', 'but', '...', 'I', 'love', 'her', 'songs', 'Dammit', '.', '_', '.', '#ArianaGrande'], ['Ariana', 'Grande', '„Äê', 'Right', 'There', 'ft', '.', 'Big', 'Sean', '„Äë', '#„Ç¢„É™„Ç¢„Éä', '#arianagrande', 'https://t.co/4rZOKYDnut'], ['which', 'one', 'would', 'you', 'prefer', 'to', 'listen', 'to', 'for', 'a', 'whole', 'day', '?', 'üòç', 'ü§ò', 'üèº', ';', 'i', 'could', 'never', 'choose', '#arianagrande', '#IntoYou', '#SideToSide', '#songs', '#Poll'], ['Booty', 'Baby', 'Ari', '#ArianaGrande', '#PrincessAri', '#bootybaby', '#DangerousWomanTour', '#DangerousWoman', '‚Ä¶', 'https://t.co/YBspYl06Xn'], ['#LindsayLohan', 'backs', 'out', 'of', 'a', '#Kettering', 'holiday', 'appearance', ',', 'just', 'after', 'throwing', 'shade', 'at', '#ArianaGrande', '.', '#LiLohttps', ':/', '/', 't.co/G2sNgjVCHd'], ['My', 'idols', 'are', '#littlemix', '#justinbieber', '#arianagrande'], ['Ariana', 'Grande', '-', 'The', 'Sims', '3', '-', 'SimsDomination', '#ArianaGrande', 'https://t.co/l2K2ihUpCg', 'https://t.co/W7sIK84nOT'], ['#Music', '#ArianaGrande-THE', 'REMIX-JAPAN', 'ONLY', '#CD', 'E78', 'https://t.co/zOxb3hwBpg', '#Bestseller', '#Hit', 'https://t.co/ibZScGfvl3'], ['#Beauty', '#ArianaGrande-CHRISTMAS', '&', 'CHILL-JAPAN', 'ONLY', '#CD', 'BONUS', 'TRACK', 'C94', 'https://t.co/pDp9gEqAW6', '#Deals', '#TopSeller', 'https://t.co/1cEhc7IqDj']]\n"
     ]
    }
   ],
   "source": [
    "# Here is NLTK's \"tweet-aware\" tokenizer\n",
    "\n",
    "nltk_casual_tokens=[]\n",
    "for tweet in tweets:\n",
    "    nltk_casual_tokens.append(nltk.casual_tokenize(tweet))\n",
    "print(nltk_casual_tokens)\n",
    "\n",
    "# See if you can spot a potential problem with one of the emoji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because language is always changing, you may still need to customize tokenization to account for what you are trying to accomplish.\n",
    "\n",
    "Below, you can see a customization of the big_regex above using compiled regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "\n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'(?:[¬∞\\p{Punctuation}\\p{Modifier_Symbol}\\p{Math_Symbol}}]+(?:[\\p{Letter}\\p{Number}][¬∞\\p{Punctuation}\\p{Modifier_Symbol}\\p{Math_Symbol}]+))',\n",
    "    r'(?:\\@+\\p{Letter}+)',\n",
    "    r'(?:\\:\\-\\p{Letter}+)',\n",
    "    r'(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)',\n",
    "    r'(?:norm__?[_A-Z]+)',\n",
    "    r'(?:\\p{Letter}\\.(?:\\p{Letter}\\.)+)',\n",
    "    r'(?:\\p{Letter}\\/\\p{Letter}(?:\\/\\p{Letter})*)',\n",
    "    #r'(?:\\b(?:(?:http|ftp|file)s?:\\/\\/\\S+\\w+\\.\\w+\\.(?:\\w\\.)?(?:com|edu|gov|org|info|biz|mil|net)?(?:[a-z]{2})?\\S+)\\b)|(?:&(?:#?[0-9a-f]+|[a-z]+);)',\n",
    "    r'(?:\\b[0-9]*(?:1st|2nd|3rd|11th|12th|13th|[4-9]th)\\b)',\n",
    "    r'(?:[.+\\-]?\\p{Number}+(?:[.,\\-:\\/]*\\p{Number}+)*)',\n",
    "    r'(?:(?:n[\\'‚Äô]t\\b)|(?:[\\'‚Äô](?:[sdm]|(?:ld)|(?:ll)|(?:re)|(?:ve)|(?:nt))\\b))',\n",
    "    r'(?:[\\p{Letter}\\p{Mark}]+(?:[\\-\\'‚Äô][\\p{Letter}\\p{Mark}]+)*)',\n",
    "    r'(?:\\.\\.+|--+|__+|~~+|!!+|\\*\\*+|\\?\\?+|//+)',\n",
    "    r'(?:\\.\\.\\.+|---+|___+|~~~+|!!!+|\\*\\*\\*+|\\?\\?\\?+|///+)',#'(?:<\\w+\\/>)',\n",
    "    r'(?:[\\@]?\\p{Letter}+)',\n",
    "#    r'(?:\\p{Punctuation}+)'\n",
    "]\n",
    "\n",
    "tokens_regex = regex.compile(r'(' + '|'.join(regex_str) + ')', regex.VERBOSE | regex.IGNORECASE)\n",
    "emoticon_regex = regex.compile(r'^' + emoticons_str + '$', regex.VERBOSE | regex.IGNORECASE)\n",
    "\n",
    "def tokenize(s):\n",
    "    return tokens_regex.findall(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey',\n",
       " '@sima',\n",
       " '#roadtrip',\n",
       " 'from',\n",
       " '09/09-9/27',\n",
       " \"aren't\",\n",
       " 'you',\n",
       " 'in',\n",
       " 'dude',\n",
       " ':-D']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if you are doing feature extraction for building a classifier, you may later want to intentionally remove tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process text\n",
    "tokens = nltk.casual_tokenize(input)\n",
    "\n",
    "punctuation = list(string.punctuation)\n",
    "\n",
    "# remove stopwords\n",
    "tokens = [term.lower() for term in tokens if term.lower() not in stopwords.words('english')]\n",
    "\n",
    "# remove punctuation\n",
    "tokens = [term for term in tokens if term not in punctuation]\n",
    "\n",
    "# remove hashtags\n",
    "tokens = [term for term in tokens if not term.startswith('#')]\n",
    "\n",
    "# remove profiles\n",
    "tokens = [term for term in tokens if not term.startswith('@')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ü•∞', 'hey', '09/09-', '9/27', \"aren't\", 'dude', ':-d']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless, you will want to compare your tokenized output with with \"gold-standard\" tokens, if possible. The NLTK corpus collection includes a sample of Penn Treebank data, including the raw Wall Street Journal text (nltk.corpus.treebank_raw.raw()) and the tokenized version (nltk.corpus.treebank.words()).\n",
    "\n",
    "Search https://www.nltk.org/genindex.html for tokenize and look closely at the number of tokenizers available.\n",
    "\n",
    "And if you look here, for example: https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.toktok.ToktokTokenizer.AMPERCENT you will see that many of the tokenizers included use lists of very carefully crafted regular expressions."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "slideshow,-all",
   "comment_magics": false,
   "formats": "ipynb,py:percent",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
