{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from: https://medium.com/analytics-vidhya/a-must-read-nlp-tutorial-on-neural-machine-translation-the-technique-powering-google-translate-c5c8d97d7587 and associated GitHub repo.\n",
    "\n",
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from numpy import array, argmax, random, take\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Bidirectional, RepeatVector, TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is a text file of English-German sentence pairs. First we will read the file using the function defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read raw text file\n",
    "def read_text(filename):\n",
    "    # open the file\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a function to split the text into English-German pairs separated by '\\n' and then split these pairs into English sentences and German sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a text into sentences\n",
    "def to_lines(text):\n",
    "    sents = text.strip().split('\\n')\n",
    "    sents = [i.split('\\t') for i in sents]\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Download the data from [here.](http://www.manythings.org/anki/deu-eng.zip)__ and extract \"deu.txt\" in your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_text(\"deu.txt\")\n",
    "deu_eng = to_lines(data)\n",
    "deu_eng = array(deu_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual data contains over 150,000 sentence-pairs. However, we will use the first 50,000 sentence pairs only to reduce the training time of the model. You can change this number as per you system computation power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "deu_eng = deu_eng[:50000,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Cleaning\n",
    "\n",
    "Let's take a look at our data, then we will decide which pre-processing steps to adopt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Hi.', 'Hallo!'],\n",
       "       ['Hi.', 'Grüß Gott!'],\n",
       "       ['Run!', 'Lauf!'],\n",
       "       ...,\n",
       "       ['Tom lost seventy pounds.', 'Tom hat 30\\xa0kg abgenommen.'],\n",
       "       ['Tom loves fried chicken.', 'Tom isst sehr gerne Brathähnchen.'],\n",
       "       ['Tom loves fried chicken.', 'Tom isst sehr gerne Brathendl.']],\n",
       "      dtype='<U302')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deu_eng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get rid of the punctuation marks, and then convert the text to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "deu_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,0]]\n",
    "deu_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Hi', 'Hallo'],\n",
       "       ['Hi', 'Grüß Gott'],\n",
       "       ['Run', 'Lauf'],\n",
       "       ...,\n",
       "       ['Tom lost seventy pounds', 'Tom hat 30\\xa0kg abgenommen'],\n",
       "       ['Tom loves fried chicken', 'Tom isst sehr gerne Brathähnchen'],\n",
       "       ['Tom loves fried chicken', 'Tom isst sehr gerne Brathendl']],\n",
       "      dtype='<U302')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deu_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert to lowercase\n",
    "for i in range(len(deu_eng)):\n",
    "    deu_eng[i,0] = deu_eng[i,0].lower()\n",
    "    \n",
    "    deu_eng[i,1] = deu_eng[i,1].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['hi', 'hallo'],\n",
       "       ['hi', 'grüß gott'],\n",
       "       ['run', 'lauf'],\n",
       "       ...,\n",
       "       ['tom lost seventy pounds', 'tom hat 30\\xa0kg abgenommen'],\n",
       "       ['tom loves fried chicken', 'tom isst sehr gerne brathähnchen'],\n",
       "       ['tom loves fried chicken', 'tom isst sehr gerne brathendl']],\n",
       "      dtype='<U302')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deu_eng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text to Sequence Conversion\n",
    "\n",
    "To feed our data in a Seq2Seq model, we will have to convert both the input and the output sentences into integer sequences of fixed length. Before that, let's visualise the length of the sentences. We will capture the lengths of all the sentences in two separate lists for English and German, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty lists\n",
    "eng_l = []\n",
    "deu_l = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in deu_eng[:,0]:\n",
    "    eng_l.append(len(i.split()))\n",
    "\n",
    "for i in deu_eng[:,1]:\n",
    "    deu_l.append(len(i.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_df = pd.DataFrame({'eng':eng_l, 'deu':deu_l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "length_df.hist(bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum length of the German sentences is 11 and that of the English phrases is 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's vectorize our text data by using Keras's Tokenizer() class. It will turn our sentences into sequences of integers. Then we will pad those sequences with zeros to make all the sequences of same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build a tokenizer\n",
    "def tokenization(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 6509\n"
     ]
    }
   ],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = tokenization(deu_eng[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "\n",
    "eng_length = 8\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deutch Vocabulary Size: 11078\n"
     ]
    }
   ],
   "source": [
    "# prepare Deutch tokenizer\n",
    "deu_tokenizer = tokenization(deu_eng[:, 1])\n",
    "deu_vocab_size = len(deu_tokenizer.word_index) + 1\n",
    "\n",
    "deu_length = 8\n",
    "print('Deutch Vocabulary Size: %d' % deu_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given below is a function to prepare the sequences. It will also perform sequence padding to a maximum sentence length as mentioned above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    seq = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    seq = pad_sequences(seq, maxlen=length, padding='post')\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now split the data into train and test set for model training and evaluation, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(deu_eng, test_size=0.2, random_state = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to encode the sentences. We will encode German sentences as the input sequences and English sentences as the target sequences. It will be done for both train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "trainX = encode_sequences(deu_tokenizer, deu_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare validation data\n",
    "testX = encode_sequences(deu_tokenizer, deu_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the exciting part! Let us define our Seq2Seq model architecture. We are using an Embedding layer and an LSTM layer as our encoder and another LSTM layer followed by a Dense layer as the decoder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build NMT model\n",
    "def build_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(RepeatVector(out_timesteps))\n",
    "    model.add(LSTM(units, return_sequences=True))\n",
    "    model.add(Dense(out_vocab, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using RMSprop optimizer in this model as it is usually a good choice for recurrent neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(deu_vocab_size, eng_vocab_size, deu_length, eng_length, 512)\n",
    "rms = optimizers.RMSprop(lr=0.001)\n",
    "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that we have used __'sparse_categorical_crossentropy'__ as the loss function because it allows us to use the target sequence as it is instead of one hot encoded format. One hot encoding the target sequences with such a huge vocabulary might consume our system's entire memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we are all set to start training our model. We will train it for 30 epochs and with a batch size of 512. You may change and play these hyperparameters. We will also be using __ModelCheckpoint()__ to save the best model with lowest validation loss. I personally prefer this method over early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marck/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/30\n",
      "32000/32000 [==============================] - 140s 4ms/step - loss: 3.6438 - val_loss: 3.0563\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.05635, saving model to model.h1.24_jan_19\n",
      "Epoch 2/30\n",
      "32000/32000 [==============================] - 139s 4ms/step - loss: 3.0002 - val_loss: 2.9585\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.05635 to 2.95847, saving model to model.h1.24_jan_19\n",
      "Epoch 3/30\n",
      "32000/32000 [==============================] - 150s 5ms/step - loss: 2.8040 - val_loss: 2.7556\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.95847 to 2.75556, saving model to model.h1.24_jan_19\n",
      "Epoch 4/30\n",
      "29696/32000 [==========================>...] - ETA: 9s - loss: 2.6362 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-ac11aca639b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m           callbacks=[checkpoint], verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filename = 'model.h1.24_jan_19'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1), \n",
    "          epochs=30, batch_size=512, \n",
    "          validation_split = 0.2,\n",
    "          callbacks=[checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the training loss and the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-314735a9fdda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train','validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the saved model to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model.h1.24_jan_19')\n",
    "preds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(n, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == n:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predictions into text (English)\n",
    "preds_text = []\n",
    "for i in preds:\n",
    "    temp = []\n",
    "    for j in range(len(i)):\n",
    "        t = get_word(i[j], eng_tokenizer)\n",
    "        if j > 0:\n",
    "            if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(t)\n",
    "             \n",
    "        else:\n",
    "            if(t == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(t)            \n",
    "        \n",
    "    preds_text.append(' '.join(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weve run out of gas</td>\n",
       "      <td>we have out  of gas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>its a fad</td>\n",
       "      <td>this is a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tom was shaken</td>\n",
       "      <td>tom was distressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this annoys me</td>\n",
       "      <td>this me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>let me do that for you</td>\n",
       "      <td>let me do   you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>you will pay for this</td>\n",
       "      <td>youll pay for this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tom often cuts classes</td>\n",
       "      <td>tom often to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hes young and healthy</td>\n",
       "      <td>he is young angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>life is enjoyable</td>\n",
       "      <td>life is beautiful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i know it sounds silly</td>\n",
       "      <td>i know it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>i raise cattle</td>\n",
       "      <td>im going to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dont worry</td>\n",
       "      <td>dont be worried</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>close the door</td>\n",
       "      <td>shut the door</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>youre joking</td>\n",
       "      <td>youre kidding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tom seemed lucky</td>\n",
       "      <td>tom seemed happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    actual                predicted\n",
       "0      weve run out of gas    we have out  of gas  \n",
       "1                its a fad           this is a     \n",
       "2           tom was shaken  tom was distressed     \n",
       "3           this annoys me            this me      \n",
       "4   let me do that for you        let me do   you  \n",
       "5    you will pay for this   youll pay for this    \n",
       "6   tom often cuts classes        tom often to     \n",
       "7    hes young and healthy    he is young angry    \n",
       "8        life is enjoyable   life is beautiful     \n",
       "9   i know it sounds silly           i know it     \n",
       "10          i raise cattle         im going to     \n",
       "11              dont worry     dont be worried     \n",
       "12          close the door       shut the door     \n",
       "13            youre joking      youre kidding      \n",
       "14        tom seemed lucky    tom seemed happy     "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>tom knew the risks</td>\n",
       "      <td>tom was all of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>what does she do</td>\n",
       "      <td>what does you do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>she still loved him</td>\n",
       "      <td>she still him sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>i want to be careful</td>\n",
       "      <td>i want to be boston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>im here arent i</td>\n",
       "      <td>im here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>youre a hypocrite</td>\n",
       "      <td>youre a hypocrite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>i gave you a book</td>\n",
       "      <td>i gave you a book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>lets do it</td>\n",
       "      <td>do  it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>i broke my leg skiing</td>\n",
       "      <td>i broke my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>i was victorious</td>\n",
       "      <td>i was been asleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>what a pretty woman</td>\n",
       "      <td>what a wife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>well come with you</td>\n",
       "      <td>were come with you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>i am the same age</td>\n",
       "      <td>im am the same age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>who should we believe</td>\n",
       "      <td>who do we going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>whats it good for</td>\n",
       "      <td>whats she good for</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     actual               predicted\n",
       "9985     tom knew the risks      tom was all of    \n",
       "9986       what does she do    what does you do    \n",
       "9987    she still loved him  she still him sing    \n",
       "9988   i want to be careful  i want to be boston   \n",
       "9989        im here arent i           im here      \n",
       "9990      youre a hypocrite  youre a hypocrite     \n",
       "9991      i gave you a book    i gave you a book   \n",
       "9992             lets do it             do  it     \n",
       "9993  i broke my leg skiing         i broke my     \n",
       "9994       i was victorious   i was been asleep    \n",
       "9995    what a pretty woman        what a wife     \n",
       "9996     well come with you  were come with you    \n",
       "9997      i am the same age   im am the same age   \n",
       "9998  who should we believe     who do we going    \n",
       "9999      whats it good for  whats she good for    "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>tom knew the risks</td>\n",
       "      <td>tom was all of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>what does she do</td>\n",
       "      <td>what does you do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>she still loved him</td>\n",
       "      <td>she still him sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>i want to be careful</td>\n",
       "      <td>i want to be boston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>im here arent i</td>\n",
       "      <td>im here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>youre a hypocrite</td>\n",
       "      <td>youre a hypocrite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>i gave you a book</td>\n",
       "      <td>i gave you a book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>lets do it</td>\n",
       "      <td>do  it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>i broke my leg skiing</td>\n",
       "      <td>i broke my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>i was victorious</td>\n",
       "      <td>i was been asleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>what a pretty woman</td>\n",
       "      <td>what a wife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>well come with you</td>\n",
       "      <td>were come with you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>i am the same age</td>\n",
       "      <td>im am the same age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>who should we believe</td>\n",
       "      <td>who do we going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>whats it good for</td>\n",
       "      <td>whats she good for</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     actual               predicted\n",
       "9985     tom knew the risks      tom was all of    \n",
       "9986       what does she do    what does you do    \n",
       "9987    she still loved him  she still him sing    \n",
       "9988   i want to be careful  i want to be boston   \n",
       "9989        im here arent i           im here      \n",
       "9990      youre a hypocrite  youre a hypocrite     \n",
       "9991      i gave you a book    i gave you a book   \n",
       "9992             lets do it             do  it     \n",
       "9993  i broke my leg skiing         i broke my     \n",
       "9994       i was victorious   i was been asleep    \n",
       "9995    what a pretty woman        what a wife     \n",
       "9996     well come with you  were come with you    \n",
       "9997      i am the same age   im am the same age   \n",
       "9998  who should we believe     who do we going    \n",
       "9999      whats it good for  whats she good for    "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4719</th>\n",
       "      <td>id like to know why</td>\n",
       "      <td>id like to know why</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6533</th>\n",
       "      <td>i dont need a car</td>\n",
       "      <td>i dont need a car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8505</th>\n",
       "      <td>youve got a fever</td>\n",
       "      <td>you have a fever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6081</th>\n",
       "      <td>dont come again</td>\n",
       "      <td>dont come again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5615</th>\n",
       "      <td>im tired of boston</td>\n",
       "      <td>im am boston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>i mean no disrespect</td>\n",
       "      <td>i dont want to be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>take a number</td>\n",
       "      <td>take a number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3561</th>\n",
       "      <td>what does it say</td>\n",
       "      <td>what does it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>shes still young</td>\n",
       "      <td>she is young</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7759</th>\n",
       "      <td>youre out of order</td>\n",
       "      <td>youre not normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>get lost</td>\n",
       "      <td>get away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6900</th>\n",
       "      <td>tom wouldnt blame you</td>\n",
       "      <td>tom wouldnt blame you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4161</th>\n",
       "      <td>youre in love</td>\n",
       "      <td>are  love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>he noticed i was there</td>\n",
       "      <td>he saw ill right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2987</th>\n",
       "      <td>tom rarely smiled</td>\n",
       "      <td>tom seldom smiled</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      actual                  predicted\n",
       "4719     id like to know why     id like to know why   \n",
       "6533       i dont need a car       i dont need a car   \n",
       "8505       youve got a fever       you have a fever    \n",
       "6081         dont come again       dont come again     \n",
       "5615      im tired of boston          im am boston     \n",
       "868     i mean no disrespect       i dont want to be   \n",
       "1415           take a number         take a number     \n",
       "3561        what does it say          what does it     \n",
       "1292        shes still young          she is young     \n",
       "7759      youre out of order      youre not normal     \n",
       "1285                get lost             get away      \n",
       "6900   tom wouldnt blame you  tom wouldnt blame you    \n",
       "4161           youre in love             are  love     \n",
       "1427  he noticed i was there       he saw ill right    \n",
       "2987       tom rarely smiled     tom seldom smiled     "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.sample(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
