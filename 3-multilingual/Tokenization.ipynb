{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "pattern1 = r'''(?x)     # set flag to allow verbose regexps**\n",
    "     (?:[A-Z]\\.)+       # abbreviations, e.g. U.S.A.\n",
    "     | \\w+(?:-\\w+)*       # words with optional internal hyphens\n",
    "     | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
    "'''\n",
    "\n",
    "pattern2 = r'''(?x)\n",
    "      (?:[A-Z]\\.)+     # abbreviations, e.g. U.S.A\n",
    "      | \\w+(?:-\\w+)*   # words with internal hyphens\n",
    "      | \\$?[0-9]+[,-\\.]?[0-9]   # currency and percentages e.g., $12.40, 83%\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"ü•∞ Hey!  @sima #roadtrip from 09/09-9/27, aren't you in dude :-D  ?!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(pattern2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', 'sima', 'roadtrip', 'from', '09', '09-9', '27', 'aren', 't', 'you', 'in', 'dude', 'd']\n"
     ]
    }
   ],
   "source": [
    "my_tokens = tokenizer.tokenize(input)\n",
    "my_normalized = [word.lower() for word in my_tokens]\n",
    "print(my_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAT?  There is disappearing text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "patterns = [\n",
    "\"(?:[¬∞\\p{Punctuation}\\p{Modifier_Symbol}\\p{Math_Symbol}}]+(?:[\\p{Letter}\\p{Number}][¬∞\\p{Punctuation}\\p{Modifier_Symbol}\\p{Math_Symbol}]+))\",\n",
    "\"(?:\\@+\\p{Letter}+)\",\n",
    "\"(?:\\:\\-\\p{Letter}+)\",\n",
    "\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",\n",
    "\"(?:norm__?[_A-Z]+)\",\n",
    "\"(?:\\p{Letter}\\.(?:\\p{Letter}\\.)+)\",\n",
    "\"(?:\\p{Letter}\\/\\p{Letter}(?:\\/\\p{Letter})*)\",\n",
    "\"(?:\\b(?:(?:http|ftp|file)s?:\\/\\/\\S+\\w+\\.\\w+\\.(?:\\w\\.)?(?:com|edu|gov|org|info|biz|mil|net)?(?:[a-z]{2})?\\S+)\\b)|(?:&(?:#?[0-9a-f]+|[a-z]+);)\",\n",
    "\"(?:\\b[0-9]*(?:1st|2nd|3rd|11th|12th|13th|[4-9]th)\\b)\",\n",
    "\"(?:[.+\\-]?\\p{Number}+(?:[.,\\-:\\/]*\\p{Number}+)*)\",\n",
    "\"(?:(?:n[\\'‚Äô]t\\b)|(?:[\\'‚Äô](?:[sdm]|(?:ld)|(?:ll)|(?:re)|(?:ve)|(?:nt))\\b))\",\n",
    "\"(?:[\\p{Letter}\\p{Mark}]+(?:[\\-\\'‚Äô][\\p{Letter}\\p{Mark}]+)*)\",\n",
    "\"(?:\\.\\.+|--+|__+|~~+|!!+|\\*\\*+|\\?\\?+|//+)\",\n",
    "\"(?:\\.\\.\\.+|---+|___+|~~~+|!!!+|\\*\\*\\*+|\\?\\?\\?+|///+)\",#\"(?:<\\w+\\/>)\",\n",
    "\"(?:[\\@]?\\p{Letter}+)\",\n",
    "\"(?:\\p{Space}+)\",\n",
    "\"(?:\\p{Separator}+)\",\n",
    "\"(?:\\p{Punctuation}+)\"\n",
    "]\n",
    "big_regex = ('|').join(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'Hey', '!', '  ', '@sima', ' ', '#roadtrip', ' ', 'from', ' ', '09/09-9/27', ',', ' ', \"aren't\", ' ', 'you', ' ', 'in', ' ', 'dude', ' ', ':-D', '  ', '?!']\n"
     ]
    }
   ],
   "source": [
    "#tokenizer = RegexpTokenizer(pattern3)\n",
    "tokens = regex.findall(big_regex,input)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Unicode hex codes for hard-to-type characters\n",
    "(AND EMOJIS)\n",
    "\n",
    "https://r12a.github.io/uniview/#title\n",
    "    \n",
    "    \"\\u231a\" for 4 digit hex, with a lowercase 'u'\n",
    "    \"\\U0001f430\" for 5 to 8 digit hex, with an uppercase 'U', pad with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when ‚åö eh Œî? üê∞  woo ü§æüèΩ‚Äç‚ôÄÔ∏è‚Äçü§æüèΩ‚Äç‚ôÄÔ∏è\n",
      "['when', '‚åö', 'eh', 'Œî', '?', 'üê∞', 'woo', 'ü§æüèΩ\\u200d‚ôÄÔ∏è\\u200dü§æüèΩ\\u200d‚ôÄÔ∏è']\n"
     ]
    }
   ],
   "source": [
    "myinput = \"when ‚åö eh Œî? üê∞  woo ü§æüèΩ‚Äç‚ôÄÔ∏è‚Äçü§æüèΩ‚Äç‚ôÄÔ∏è\"\n",
    "import regex\n",
    "patterns = [\n",
    "    \"(?:\\p{Other_Symbol}[\\U0001f3fb-\\U0001f3ff]?\\uFE0F?(?:\\u200D\\p{Other_Symbol}[\\U0001f3fb-\\U0001f3ff]?\\uFE0F?)*)\",\n",
    "    \"\\u231a\",\n",
    "    \"\\U00000394\",\n",
    "    \"\\U0001f430\",\n",
    "    \"\\p{Letter}+\",\n",
    "    \"\\p{Punctuation}\"\n",
    "]\n",
    "big_regex = ('|').join(patterns)\n",
    "tokens = regex.findall(big_regex,myinput)\n",
    "print(myinput)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO I REALLY have to maintain this gigantic regex?!\n",
    "\n",
    "No.  And you probably shouldn't.  But they are handy for various things such as token tests.  How about trying stanfordnlp's tokenizers and part of speech taggers?\n",
    "\n",
    "## Stanford NLP - multi-language tokenization, lemmatization, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/Users/lisa/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/Users/lisa/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/Users/lisa/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/Users/lisa/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "Done loading processors!\n",
      "---\n",
      "<Token index=1;words=[<Word index=1;text=Barack;lemma=Barack;upos=PROPN;xpos=NNP;feats=Number=Sing>]>\n",
      "<Token index=2;words=[<Word index=2;text=Obama;lemma=Obama;upos=PROPN;xpos=NNP;feats=Number=Sing>]>\n",
      "<Token index=3;words=[<Word index=3;text=was;lemma=be;upos=AUX;xpos=VBD;feats=Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin>]>\n",
      "<Token index=4;words=[<Word index=4;text=born;lemma=bear;upos=VERB;xpos=VBN;feats=Tense=Past|VerbForm=Part|Voice=Pass>]>\n",
      "<Token index=5;words=[<Word index=5;text=in;lemma=in;upos=ADP;xpos=IN;feats=_>]>\n",
      "<Token index=6;words=[<Word index=6;text=Hawaii;lemma=Hawaii;upos=PROPN;xpos=NNP;feats=Number=Sing>]>\n",
      "<Token index=7;words=[<Word index=7;text=.;lemma=.;upos=PUNCT;xpos=.;feats=_>]>\n"
     ]
    }
   ],
   "source": [
    "pipeline_settings = \"tokenize,mwt,pos,lemma\"\n",
    "nlp = stanfordnlp.Pipeline(lang='en',processors=pipeline_settings) \n",
    "doc = nlp(\"Barack Obama was born in Hawaii.  He was elected president in 2008.\")\n",
    "doc.sentences[0].print_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't have this resource, you may need to download\n",
    "# stanfordnlp.download('ar') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/Users/lisa/stanfordnlp_resources/ar_padt_models/ar_padt_tokenizer.pt', 'lang': 'ar', 'shorthand': 'ar_padt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: mwt\n",
      "With settings: \n",
      "{'model_path': '/Users/lisa/stanfordnlp_resources/ar_padt_models/ar_padt_mwt_expander.pt', 'lang': 'ar', 'shorthand': 'ar_padt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/Users/lisa/stanfordnlp_resources/ar_padt_models/ar_padt_tagger.pt', 'pretrain_path': '/Users/lisa/stanfordnlp_resources/ar_padt_models/ar_padt.pretrain.pt', 'lang': 'ar', 'shorthand': 'ar_padt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/Users/lisa/stanfordnlp_resources/ar_padt_models/ar_padt_lemmatizer.pt', 'lang': 'ar', 'shorthand': 'ar_padt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "Done loading processors!\n",
      "---\n",
      "<Token index=1;words=[<Word index=1;text=ÿ£ÿ≠ÿ®;lemma=ÿ£Ÿéÿ≠Ÿéÿ®;upos=VERB;xpos=VIIA-3MS--;feats=Aspect=Imp|Gender=Masc|Mood=Ind|Number=Sing|Person=1|VerbForm=Fin|Voice=Act>]>\n",
      "<Token index=2;words=[<Word index=2;text=ÿßŸÑÿ™ÿ¨ÿØŸäŸÅ;lemma=ÿ™Ÿéÿ¨ÿØŸêŸäŸÅ;upos=NOUN;xpos=N------S4D;feats=Case=Acc|Definite=Def|Number=Sing>]>\n",
      "<Token index=3;words=[<Word index=3;text=ÿπŸÑŸâ;lemma=ÿπŸéŸÑŸéŸâ;upos=ADP;xpos=P---------;feats=AdpType=Prep>]>\n",
      "<Token index=4;words=[<Word index=4;text=ŸÜŸáÿ±;lemma=ŸÜŸéŸáÿ±;upos=NOUN;xpos=N------S2R;feats=Case=Gen|Definite=Cons|Number=Sing>]>\n",
      "<Token index=5;words=[<Word index=5;text=ÿ®ŸÑÿØŸä;lemma=ÿ®ŸÑÿØŸä;upos=X;xpos=U---------;feats=_>]>\n",
      "<Token index=6;words=[<Word index=6;text=.;lemma=.;upos=PUNCT;xpos=G---------;feats=_>]>\n"
     ]
    }
   ],
   "source": [
    "pipeline_settings = \"tokenize,mwt,pos,lemma\"\n",
    "nlp = stanfordnlp.Pipeline(lang='ar',processors=pipeline_settings) \n",
    "doc = nlp(\"ÿ£ÿ≠ÿ® ÿßŸÑÿ™ÿ¨ÿØŸäŸÅ ÿπŸÑŸâ ŸÜŸáÿ± ÿ®ŸÑÿØŸä.\")\n",
    "doc.sentences[0].print_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "StanfordNLP Tutorial and overview:  https://www.analyticsvidhya.com/blog/2019/02/stanfordnlp-nlp-library-python/\n",
    "\n",
    "Stanford NLP neural etc.  https://github.com/stanfordnlp/\n",
    "\n",
    "Stanford models:  https://stanfordnlp.github.io/stanfordnlp/models.html#human-languages-supported-by-stanfordnlp\n",
    "\n",
    "Tagset conversions for specific treebanks to Universal Dependencies: https://stanfordnlp.github.io/stanfordnlp/models.html#human-languages-supported-by-stanfordnlp\n",
    "\n",
    "A nice article on Chinese segmentation:  https://medium.com/the-artificial-impostor/nlp-four-ways-to-tokenize-chinese-documents-f349eb6ba3c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "comment_magics": false,
   "encoding": "# -*- coding: utf-8 -*-",
   "text_representation": {
    "extension": ".py",
    "format_name": "hydrogen",
    "format_version": "1.2",
    "jupytext_version": "1.1.5"
   }
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
