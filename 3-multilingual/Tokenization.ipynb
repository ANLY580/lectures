{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern1 = r'''(?x)     # set flag to allow verbose regexps**\n",
    "     (?:[A-Z]\\.)+       # abbreviations, e.g. U.S.A.\n",
    "     | \\w+(?:-\\w+)*       # words with optional internal hyphens\n",
    "     | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
    "'''\n",
    "\n",
    "pattern2 = r'''(?x)\n",
    "      (?:[A-Z]\\.)+     # abbreviations, e.g. U.S.A\n",
    "      | \\w+(?:-\\w+)*   # words with internal hyphens\n",
    "      | \\$?[0-9]+[,-\\.]?[0-9]   # currency and percentages e.g., $12.40, 83%\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"ü•∞ Hey!  @sima #roadtrip from 09/09-9/27, aren't you in dude :-D  ?!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(pattern2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokens = tokenizer.tokenize(input)\n",
    "my_normalized = [word.lower() for word in my_tokens]\n",
    "print(my_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAT?  There is disappearing text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "patterns = [\n",
    "\"(?:[¬∞\\p{Punctuation}\\p{Modifier_Symbol}\\p{Math_Symbol}}]+(?:[\\p{Letter}\\p{Number}][¬∞\\p{Punctuation}\\p{Modifier_Symbol}\\p{Math_Symbol}]+))\",\n",
    "\"(?:\\@+\\p{Letter}+)\",\n",
    "\"(?:\\:\\-\\p{Letter}+)\",\n",
    "\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",\n",
    "\"(?:norm__?[_A-Z]+)\",\n",
    "\"(?:\\p{Letter}\\.(?:\\p{Letter}\\.)+)\",\n",
    "\"(?:\\p{Letter}\\/\\p{Letter}(?:\\/\\p{Letter})*)\",\n",
    "\"(?:\\b(?:(?:http|ftp|file)s?:\\/\\/\\S+\\w+\\.\\w+\\.(?:\\w\\.)?(?:com|edu|gov|org|info|biz|mil|net)?(?:[a-z]{2})?\\S+)\\b)|(?:&(?:#?[0-9a-f]+|[a-z]+);)\",\n",
    "\"(?:\\b[0-9]*(?:1st|2nd|3rd|11th|12th|13th|[4-9]th)\\b)\",\n",
    "\"(?:[.+\\-]?\\p{Number}+(?:[.,\\-:\\/]*\\p{Number}+)*)\",\n",
    "\"(?:(?:n[\\'‚Äô]t\\b)|(?:[\\'‚Äô](?:[sdm]|(?:ld)|(?:ll)|(?:re)|(?:ve)|(?:nt))\\b))\",\n",
    "\"(?:[\\p{Letter}\\p{Mark}]+(?:[\\-\\'‚Äô][\\p{Letter}\\p{Mark}]+)*)\",\n",
    "\"(?:\\.\\.+|--+|__+|~~+|!!+|\\*\\*+|\\?\\?+|//+)\",\n",
    "\"(?:\\.\\.\\.+|---+|___+|~~~+|!!!+|\\*\\*\\*+|\\?\\?\\?+|///+)\",#\"(?:<\\w+\\/>)\",\n",
    "\"(?:[\\@]?\\p{Letter}+)\",\n",
    "\"(?:\\p{Space}+)\",\n",
    "\"(?:\\p{Separator}+)\",\n",
    "\"(?:\\p{Punctuation}+)\"\n",
    "]\n",
    "big_regex = ('|').join(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = RegexpTokenizer(pattern3)\n",
    "tokens = regex.findall(big_regex,input)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Unicode hex codes for hard-to-type characters\n",
    "(AND EMOJIS)\n",
    "\n",
    "https://r12a.github.io/uniview/#title\n",
    "    \n",
    "    \"\\u231a\" for 4 digit hex, with a lowercase 'u'\n",
    "    \"\\U0001f430\" for 5 to 8 digit hex, with an uppercase 'U', pad with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myinput = \"when ‚åö eh Œî? üê∞  woo ü§æüèΩ‚Äç‚ôÄÔ∏è‚Äçü§æüèΩ‚Äç‚ôÄÔ∏è\"\n",
    "import regex\n",
    "patterns = [\n",
    "    \"(?:\\p{Other_Symbol}[\\U0001f3fb-\\U0001f3ff]?\\uFE0F?(?:\\u200D\\p{Other_Symbol}[\\U0001f3fb-\\U0001f3ff]?\\uFE0F?)*)\",\n",
    "    \"\\u231a\",\n",
    "    \"\\U00000394\",\n",
    "    \"\\U0001f430\",\n",
    "    \"\\p{Letter}+\",\n",
    "    \"\\p{Punctuation}\"\n",
    "]\n",
    "big_regex = ('|').join(patterns)\n",
    "tokens = regex.findall(big_regex,myinput)\n",
    "print(myinput)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO I REALLY have to maintain this gigantic regex?!\n",
    "\n",
    "No.  And you probably shouldn't.  But they are handy for various things such as token tests.  How about trying stanfordnlp's tokenizers and part of speech taggers?\n",
    "\n",
    "## Stanford NLP - multi-language tokenization, lemmatization, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_settings = \"tokenize,mwt,pos,lemma\"\n",
    "nlp = stanfordnlp.Pipeline(lang='en',processors=pipeline_settings) \n",
    "doc = nlp(\"Barack Obama was born in Hawaii.  He was elected president in 2008.\")\n",
    "doc.sentences[0].print_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_settings = \"tokenize,mwt,pos,lemma\"\n",
    "nlp = stanfordnlp.Pipeline(lang='ar',processors=pipeline_settings) \n",
    "doc = nlp(\"ÿ£ÿ≠ÿ® ÿßŸÑÿ™ÿ¨ÿØŸäŸÅ ÿπŸÑŸâ ŸÜŸáÿ± ÿ®ŸÑÿØŸä.\")\n",
    "doc.sentences[0].print_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "StanfordNLP Tutorial and overview:  https://www.analyticsvidhya.com/blog/2019/02/stanfordnlp-nlp-library-python/\n",
    "\n",
    "Stanford NLP neural etc.  https://github.com/stanfordnlp/\n",
    "\n",
    "Stanford models:  https://stanfordnlp.github.io/stanfordnlp/models.html#human-languages-supported-by-stanfordnlp\n",
    "\n",
    "Tagset conversions for specific treebanks to Universal Dependencies: https://stanfordnlp.github.io/stanfordnlp/models.html#human-languages-supported-by-stanfordnlp\n",
    "\n",
    "A nice article on Chinese segmentation:  https://medium.com/the-artificial-impostor/nlp-four-ways-to-tokenize-chinese-documents-f349eb6ba3c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
