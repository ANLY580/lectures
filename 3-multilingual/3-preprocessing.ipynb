{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "---\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    cell_markers: region,endregion\n",
    "    comment_magics: false\n",
    "    formats: ipynb,.pct.py:hydrogen,Rmd,md\n",
    "    text_representation:\n",
    "      extension: .py\n",
    "      format_name: hydrogen\n",
    "      format_version: '1.1'\n",
    "      jupytext_version: 1.1.5\n",
    "  kernelspec:\n",
    "    display_name: Python 3\n",
    "    language: python\n",
    "    name: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# If you are working in binder, you can comment any import statements in the blocks below.\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# This is the file from last week.\n",
    "\n",
    "with open('ca11.txt') as file:\n",
    "    print(file.read(20))\n",
    "    ca11_raw = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Recall how difficult it was to tokenize using a single regular expression and that we \n",
    "# couldn't even get all of the number formats in ca11.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'''(?x)     # set flag to allow verbose regexps**\n",
    "     (?:[A-Z]\\.)+       # abbreviations, e.g. U.S.A.\n",
    "     | \\w+(?:-\\w+)*       # words with optional internal hyphens\n",
    "     | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
    "'''\n",
    "\n",
    "tokenizer = RegexpTokenizer(pattern)\n",
    "my_ca11_tokens = tokenizer.tokenize(ca11_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ca11_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reality is that you will want to compare your tokenized output with with \"gold-standard\" tokens, if possible. The NLTK corpus collection includes a sample of Penn Treebank data, including the raw Wall Street Journal text (nltk.corpus.treebank_raw.raw()) and the tokenized version (nltk.corpus.treebank.words()).\n",
    "\n",
    "Search https://www.nltk.org/genindex.html for tokenize and look closely at the number of tokenizers available.\n",
    "\n",
    "And if you look here, for example: https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.toktok.ToktokTokenizer.AMPERCENT you will see that many of the tokenizers included use lists of very carefully crafted regular expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: compiled regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of advantages and disadvantages to tokenizers in NLTK. For example, \n",
    "- ToktokTokenizer() is very fast\n",
    "- MosesTokenizer() is backwards capable and can detokenize text\n",
    "- ReppTokenizer() is able to provide token offsets\n",
    "\n",
    "Actually - the MosesTokenizer seems to have been moved to: https://github.com/alvations/sacremoses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing gears to short texts like those in twitter or other social media, there are some patterns that require specialized tokenizers and pre-processing steps.\n",
    "\n",
    "Below are the first few tweets from assignment 1 - Twitter English data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('social.txt', encoding=\"utf-8\") as file:\n",
    "    tweets=[]\n",
    "    data = file.readlines()\n",
    "    for tweet in data:\n",
    "        tweets.append(tweet)\n",
    "    print(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even for English, there are potentially number of challenges for tokenization:\n",
    "- mentions/usernames\n",
    "- URLs, numbers\n",
    "- textual [emoticons](https://en.wikipedia.org/wiki/List_of_emoticons)\n",
    "- [emoji](https://en.wikipedia.org/wiki/Emoji)\n",
    "- words (including hyphenated words)\n",
    "- case-folding \n",
    "- punctuation \n",
    "- hashtags \n",
    "- non-English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's check out the Whitespace Tokenizer from last week\n",
    "whitespace_tokens=[]\n",
    "for tweet in tweets:\n",
    "    whitespace_tokens.append(tweet.split())\n",
    "print(whitespace_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all of these tokenizers use **compiled regular expressions** which are cached and may result in substantial performance gain, depending on how often you use them and possibly how many you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK also has a \"tweet-aware\" tokenizer with some options\n",
    "# https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.casual.casual_tokenize\n",
    "\n",
    "nltk_casual_tokens=[]\n",
    "for tweet in tweets:\n",
    "    nltk_casual_tokens.append(nltk.casual_tokenize(tweet))\n",
    "print(nltk_casual_tokens)\n",
    "\n",
    "# How does this differ from what you see above? One way to test is to compare tokens\n",
    "# that don't occur from one to the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO: re.UNICODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A somewhat better way to write a tokenizer with multiple\n",
    "# regular expressions is in this snippet below (shorter version of \n",
    "# http://sentiment.christopherpotts.net/code-data/happyfuntokenizing.py)\n",
    "\n",
    "# The order is important (match from first to last)\n",
    "\n",
    "# Keep usernames together (any token starting with @, followed by A-Z, a-z, 0-9)\n",
    "regexes=(r\"(?:@[\\w_]+)\",\n",
    "\n",
    "# Keep hashtags together (any token starting with #, followed by A-Z, a-z, 0-9, _, or -)\n",
    "r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",\n",
    "\n",
    "# Keep words with apostrophes, hyphens and underscores together\n",
    "r\"(?:[a-z][a-z’'\\-_]+[a-z])\",\n",
    "\n",
    "# Keep all other sequences of A-Z, a-z, 0-9, _ together\n",
    "r\"(?:[\\w_]+)\",\n",
    "\n",
    "# Everything else that's not whitespace\n",
    "r\"(?:\\S)\"\n",
    ")\n",
    "\n",
    "big_regex=\"|\".join(regexes)\n",
    "\n",
    "my_extensible_tokenizer = re.compile(big_regex, re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "def my_extensible_tokenize(text):\n",
    "    return my_extensible_tokenizer.findall(text)\n",
    "\n",
    "# Note re.I for performing Perform case-insensitive matching; \n",
    "# expressions like [A-Z] will match lowercase letters, too. \n",
    "# You get the same effect onnon-ASCII Unicode characters such as ü and Ü, \n",
    "# by adding the UNICODE flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extensible_tokens=[]\n",
    "for tweet in tweets:\n",
    "    extensible_tokens.append(my_extensible_tokenize(tweet))\n",
    "print(extensible_tokens)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "slideshow,-all",
   "formats": "ipynb,md",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".md",
    "format_name": "markdown"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
