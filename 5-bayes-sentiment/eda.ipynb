{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "import string\n",
    "from collections import Counter\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import casual_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import bigrams\n",
    "from nltk.lm import NgramCounter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/'\n",
    "DATA = 'INPUT.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "punctuation = list(string.punctuation) + ['…', '...','’']\n",
    "stop_words = stopwords.words('english') + ['rt', 'via']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# The total number of distinct words (vocabulary)\n",
    "# The total number of tokens corresponding to the top 10 most frequent words (types) in the vocabulary\n",
    "def totals(text):\n",
    "    token_count = 0\n",
    "    counts = Counter(text)\n",
    "    for key, val in counts.most_common(10):\n",
    "        token_count += val\n",
    "    return len(counts), counts.most_common(10), token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# The number of words, characters and av char count for each tweet\n",
    "def tweet_counts(tweet):\n",
    "    char_counts = [len(word) for word in tweet]\n",
    "    total_chars = sum(char_counts)\n",
    "    av_chars = total_chars / len(char_counts)\n",
    "    return len(tweet), total_chars, av_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# calculate standard deviation of characters per token in a tweet\n",
    "def standard_deviation(text):\n",
    "    char_count = []\n",
    "    words = text.split(' ')\n",
    "    for word in words:\n",
    "        char_count.append(len(word))\n",
    "\n",
    "    diffs = 0\n",
    "    average = sum(char_count)/len(char_count)\n",
    "    for n in char_count:\n",
    "        diffs += (n - average)**(2)\n",
    "    return (diffs/(len(char_count)-1))**(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# The average number and standard deviation of characters per token\n",
    "# The total number of characters\n",
    "def token_counts(tweet_list):\n",
    "    # sum the values with same keys for char_count and av_char_count\n",
    "    char_counts = []\n",
    "    word_counts = []\n",
    "\n",
    "    for tweet in tweet_list:\n",
    "        char_counts.append(tweet['char_count'])\n",
    "        word_counts.append(tweet['word_count'])\n",
    "\n",
    "    sd = standard_deviation(tweet['text'])\n",
    "    total_chars = sum(char_counts)\n",
    "    total_words = sum(word_counts)\n",
    "    av_char_count = total_chars / total_words\n",
    "    return total_chars, av_char_count, sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# debugging\n",
    "def write_counter_to_file(counters):\n",
    "    filenames = ['bigrams','trigrams','fourgrams','fivegrams']\n",
    "    num = 0\n",
    "    for counter in counters:\n",
    "        filename = filenames[num]\n",
    "        with open(filename + '.csv', 'w') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            for key, value in counter.items():\n",
    "                writer.writerow(list(key) + [value])\n",
    "            num = num+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "        # The total number of distinct n-grams (of words) that appear in the dataset for n=2,3\n",
    "# You can use sets or Counters pretty easily\n",
    "# https://dbader.org/blog/sets-and-multiset-in-python\n",
    "# Check http://www.nltk.org/api/nltk.html#nltk.util.bigrams\n",
    "# This also looks promising\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "def ngram_counts(tweet_list):\n",
    "    bigram_counter = Counter()\n",
    "    trigram_counter = Counter()\n",
    "    fourgram_counter = Counter()\n",
    "    fivegram_counter = Counter()\n",
    "    list_counters = []\n",
    "\n",
    "    for tweet in tweet_list:\n",
    "        tokens = tweet['text'].split()\n",
    "        bigrams = list(nltk.ngrams(tokens, 2))\n",
    "        trigrams = list(nltk.ngrams(tokens, 3))\n",
    "        fourgrams = list(nltk.ngrams(tokens, 4))\n",
    "        fivegrams = list(nltk.ngrams(tokens, 5))\n",
    "        bigram_counter.update(bigrams)\n",
    "        trigram_counter.update(trigrams)\n",
    "        fourgram_counter.update(fourgrams)\n",
    "        fivegram_counter.update(fivegrams)\n",
    "    distinct_bigram_counter = len(bigram_counter) # distinct counts\n",
    "    distinct_trigram_counter = len(trigram_counter) # distinct counts\n",
    "    distinct_fourgram_counter = len(fourgram_counter) # distinct counts\n",
    "    distinct_fivegram_counter = len(fivegram_counter) # distinct counts\n",
    "\n",
    "    # debug... trigrams count slightly higher??\n",
    "    list_counters = [bigram_counter, trigram_counter, fourgram_counter, fivegram_counter]\n",
    "    write_counter_to_file(list_counters)\n",
    "\n",
    "    #total_counter = sum(ngram_counter.values()) # total counts\n",
    "    return distinct_bigram_counter, distinct_trigram_counter, distinct_fourgram_counter, distinct_fivegram_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a token log frequency. Describe what this plot means and how to interpret it.\n",
    "A useful way to do this is by plotting log-frequency against the log-rank\n",
    "You could also plot Heap's law - types versus tokens\n",
    "You could also do a log frequency graph of the top n tokens\n",
    "Describe out it might help you understand coverage when training a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def token_log_freq(corpus):\n",
    "    corpus_counts = Counter(corpus)\n",
    "    plt.loglog([val for word, val in corpus_counts.most_common(1000)])\n",
    "    plt.xlabel('rank')\n",
    "    plt.ylabel('frequency')\n",
    "    #plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_DIR + DATA, 'r') as data:\n",
    "    tweet_list = []\n",
    "    corpus = []\n",
    "    for line in data:\n",
    "        #print(line.split('\\t'))\n",
    "\n",
    "        info = line.strip().split('\\t')\n",
    "        id, label, text = info[0], info[1], ' '.join(info[2:])\n",
    "\n",
    "        #tokens = tokenize(text)\n",
    "        tokens = casual_tokenize(text)\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        tokens = [token for token in tokens if not token in stop_words]\n",
    "        tokens = [token for token in tokens if not token in punctuation]\n",
    "        #tokens = [tokens for token in tokens if not token.isdigit()]\n",
    "\n",
    "        corpus.extend(tokens) # one giant list\n",
    "        word_count, char_count, av_char_count = tweet_counts(tokens)\n",
    "\n",
    "        row = {\n",
    "            'id': id,\n",
    "            'label': label,\n",
    "            'text': ' '.join(tokens),\n",
    "            'word_count': word_count,\n",
    "            'char_count': char_count,\n",
    "        }\n",
    "        tweet_list.append(row)\n",
    "\n",
    "        total_tweets = len(tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_chars, av_char_count, sd = token_counts(tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total tweets = \", total_tweets)\n",
    "print(\"total tokens = \", len(corpus))\n",
    "print(\"total number of characters = \", total_chars)\n",
    "print(\"av chars per token = \", av_char_count)\n",
    "print(\"sd for chars per token = \", sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, most_common, tokens_common = totals(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"vocabulary size = \", vocab)\n",
    "print(\"10 most common words = \", most_common)\n",
    "print(\"token count of 10 most common words = \", tokens_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The token/type ratio in the dataset\n",
    "# Type-Token Ratio can be obtained by dividing the total type count by the total token count.\n",
    "# The basic idea is that the higher the number, the more lexically diverse\n",
    "print(\"token/type ratio = \", vocab / len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counts, trigram_counts, fourgram_counts, fivegram_counts = ngram_counts(tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"unigram count = \", len(corpus))\n",
    "print(\"bigram count = \", bigram_counts)\n",
    "print(\"trigram count = \", trigram_counts)\n",
    "print(\"four-gram count = \", fourgram_counts)\n",
    "print(\"five-gram count = \", fivegram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the graph approximately accurate, it should look linear\n",
    "# The idea here is that word count distributions follow a power law\n",
    "# According to Zipf's law, the frequency of any word is inversely proportional\n",
    "# to its rank in the frequency table.\n",
    "token_log_freq(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to prove that the nltk ngram function is working as expected\n",
    "# The total number of trigrams is always one less than the number of bigrams, \n",
    "# but the number of unique trigrams is likely to be greater and will have lower numbers unless a lot of the bigrams \n",
    "# are wholly subsumed by the corresponding trigrams\n",
    "\n",
    "input = \"a ab abc abcd abcde abcdef abcdefg.\"\n",
    "\n",
    "bigrams = {}\n",
    "trigrams = {}\n",
    "bigrams_total = 0\n",
    "trigrams_total = 0\n",
    "\n",
    "i = 0\n",
    "while i < len(input):\n",
    "    if i+1 < len(input):\n",
    "        bigram = input[i:i+2]\n",
    "        print(\"bigram : [\" + bigram + \"]   length: \", len(bigram))\n",
    "        if bigram in bigrams:\n",
    "            bigrams[bigram] += 1\n",
    "        else:\n",
    "            bigrams[bigram] = 1\n",
    "        bigrams_total += 1\n",
    "    if i + 2 < len(input):\n",
    "        trigram = input[i:i+3]\n",
    "        print(\"trigram: [\"+ trigram + \"]    length\", len(trigram))\n",
    "        if trigram in trigrams:\n",
    "            trigrams[trigram] += 1\n",
    "        else:\n",
    "            trigrams[trigram] = 1\n",
    "        trigrams_total += 1\n",
    "    i += 1\n",
    "print(\"\\n\")\n",
    "print(\"length of input\", len(input), \"\\n\")\n",
    "print(\"bigrams length (unique bigrams): \", len(bigrams))\n",
    "print(\"bigrams total: \", bigrams_total, \"\\n\")\n",
    "print(\"trigrams length (unique trigrams): \", len(trigrams))\n",
    "print(\"trigrams total: \", trigrams_total, \"\\n\")\n",
    "print (\"\\nnum bigrams == len(input) -1\")\n",
    "print (\"num trigrams == len(input) -2\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "comment_magics": false,
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
