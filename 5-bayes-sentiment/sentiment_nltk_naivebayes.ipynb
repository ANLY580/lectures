{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "\n",
    "# Useful references:\n",
    "# https://towardsdatascience.com/basic-binary-sentiment-analysis-using-nltk-c94ba17ae386\n",
    "\n",
    "# NLTK provides for some other sorts of features\n",
    "# from nltk.sentiment.util import (mark_negation, extract_unigram_feats)\n",
    "# mark_negation(): Append _NEG suffix to words that appear in the scope between\n",
    "# a negation and a punctuation mark. extract_unigram_feats():\n",
    "# Populate a dictionary of unigram features, reflecting the presence/absence\n",
    "# in the document of each of the tokens in unigrams.\n",
    "\n",
    "# Data\n",
    "PROC_DIR = 'data/'\n",
    "TRAIN = PROC_DIR + 'train.csv'\n",
    "DEV =  PROC_DIR + 'dev.csv'\n",
    "# In a previous step, I tokenized and pre-processed data and written\n",
    "# out to a csv file.\n",
    "\n",
    "df_train = pd.read_csv(TRAIN)\n",
    "df_dev = pd.read_csv(DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(df_train,columns=['id','label','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "df_pos_train = df_train[df_train['label'] == 'positive']\n",
    "pos_tweets = df_pos_train['text'].tolist()\n",
    "\n",
    "df_neg_train = df_train[df_train['label'] == 'negative']\n",
    "neg_tweets = df_neg_train['text'].tolist()\n",
    "\n",
    "df_neutral_train = df_train[df_train['label'] == 'neutral']\n",
    "neutral_tweets = df_neutral_train['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3094"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how balanced is this training set?\n",
    "len(df_pos_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "863"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_neg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2043"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_neutral_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(sentence):\n",
    "    words = sentence.lower().split()\n",
    "    return dict(('contains(%s)' % w, True) for w in words)\n",
    "\n",
    "positive_featuresets = [(features(tweet),'positive') for tweet in pos_tweets]\n",
    "negative_featuresets = [(features(tweet),'negative') for tweet in neg_tweets]\n",
    "neutral_featuresets = [(features(tweet),'neutral') for tweet in neutral_tweets]\n",
    "training_features = positive_featuresets + negative_featuresets + neutral_featuresets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer = SentimentAnalyzer()\n",
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentiment_analyzer.train(trainer, training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1999"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create evaluation data\n",
    "\n",
    "#df_dev = pd.DataFrame(df_dev,columns=['id','label','text'])\n",
    "truth_list = list(df_dev[['text', 'label']].itertuples(index=False, name=None))\n",
    "len(truth_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('barack michelle obama walk runway style tonight tomorrow life complete',\n",
       " 'positive')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check to make sure we manipulated the dataframe properly\n",
    "truth_list[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'contains(barack)': True,\n",
       "  'contains(michelle)': True,\n",
       "  'contains(obama)': True,\n",
       "  'contains(walk)': True,\n",
       "  'contains(runway)': True,\n",
       "  'contains(style)': True,\n",
       "  'contains(tonight)': True,\n",
       "  'contains(tomorrow)': True,\n",
       "  'contains(life)': True,\n",
       "  'contains(complete)': True},\n",
       " 'positive')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The evaluation method needs the feature extractor that was run to train the classifier\n",
    "# Specifically, it wants a list of tuples (features,truth), where features is a dict\n",
    "for i, (text, expected) in enumerate(truth_list):\n",
    "    text_feats = features(text)\n",
    "    truth_list[i] = (text_feats, expected)\n",
    "truth_list[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating NaiveBayesClassifier results...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.4807403701850925,\n",
       " 'Precision [positive]': 0.6909620991253644,\n",
       " 'Recall [positive]': 0.5622775800711743,\n",
       " 'F-measure [positive]': 0.6200130804447351,\n",
       " 'Precision [negative]': 0.31548311990686845,\n",
       " 'Recall [negative]': 0.6930946291560103,\n",
       " 'F-measure [negative]': 0.4336,\n",
       " 'Precision [neutral]': 0.47577092511013214,\n",
       " 'Recall [neutral]': 0.2823529411764706,\n",
       " 'F-measure [neutral]': 0.3543888433141919}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate and print out all metrics\n",
    "sentiment_analyzer.evaluate(truth_list,classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.4807403701850925\n",
      "F-measure [negative]: 0.4336\n",
      "F-measure [neutral]: 0.3543888433141919\n",
      "F-measure [positive]: 0.6200130804447351\n",
      "Precision [negative]: 0.31548311990686845\n",
      "Precision [neutral]: 0.47577092511013214\n",
      "Precision [positive]: 0.6909620991253644\n",
      "Recall [negative]: 0.6930946291560103\n",
      "Recall [neutral]: 0.2823529411764706\n",
      "Recall [positive]: 0.5622775800711743\n"
     ]
    }
   ],
   "source": [
    "# example of how to get to individual metrics\n",
    "for key,value in sorted(sentiment_analyzer.evaluate(truth_list).items()):\n",
    "    print('{0}: {1}'.format(key, value))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "comment_magics": false
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
